% Processed Transcript: ../course_website/ModellingNeuralSystems/overheads/transcripts/Class2_raw.md
% Date: 2025-12-10

## Introduzione

Chiudiamo la porta. Come state trovando i colloquia finora? Al di là dell'aspetto logistico, li trovate utili? La settimana prossima ospiteremo una ricercatrice della SISSA, con doppia nazionalità brasiliana e italiana, che è molto orientata all'imprenditoria. Sta fondando una spin-off con me – anche se il mio ruolo è irrilevante – e ha un'idea progettuale molto ben definita e ancora nelle fasi iniziali. Ho pensato che, oltre alla sua notevole bravura, la sua presentazione potrebbe essere di ulteriore interesse per voi, non solo per l'aspetto accademico, sebbene il colloquio odierno del Professor Silvestro Micera sia stato ovviamente a 360 gradi.

Ok, allora, continuiamo oggi. Dovete tenere d'occhio l'orario, altrimenti potrei andare avanti fino alle sei di sera. Ditemi pure. [Inference] Stavo pensando che mi interessa comunicare con voi più facilmente. Potrei darvi il mio numero di telefono per WhatsApp, dato che Teams non è sempre il più agevole per la comunicazione interna. Se volete, potete aggiungermi. Avevo assunto che aveste Teams installato sui vostri telefoni, ma capisco che spesso non lo usate.

Volete aspettare un quarto d'ora per vedere se è rimasto fuori qualcun altro? Se volete, possiamo farlo. Ad esempio, il prossimo venerdì, a seconda che lo speaker se ne vada o meno, come è successo oggi, potremmo iniziare prima. Semplicemente perché oggi è una bella giornata e potrebbe valere la pena stare fuori. Quando inizierà a fare buio, a piovere, eccetera, faremo mezz'ora in più di lezione. No, scherzo.

## L'Approccio "Forza Bruta" alla Modellistica Neuronale

Oggi voglio continuare a parlarvi in modo generale, focalizzandomi su un certo tipo di modellistica, su quello che io chiamo l'approccio "forza bruta" alla modellistica neuronale. Questo approccio è sempre esistito e ha avuto una continuazione significativa con il lavoro pionieristico di Hodgkin e Huxley. Vi ricordo che non solo hanno proposto un modello di eccitabilità a singolo compartimento, ma hanno di fatto riesumato l'equazione del cavo, che avete visto e ricorderete dal corso precedente sui segnali elettrofisiologici.

Da lì, la storia di avere un cavo – nel loro caso, negli articoli pubblicati attorno al 1952, alcuni dei quali riguardavano la dimensione spazialmente distribuita – ha preso piede. Appena i computer sono diventati un po' più semplici e potenti, la gente ha continuato a sviluppare questo campo, in particolare nell'aspetto della ricostruzione della morfologia. Questo è l'argomento di cui vi parlerò e che criticherò oggi, perché, per mia formazione, sono un riduzionista. Penso, e ve l'ho già accennato, che possa essere più vantaggioso avere un approccio di modellistica dove i modelli sono semplici, facili da simulare e da comprendere, e non ugualmente complessi come il sistema biologico. Al di là del fatto che un modello non lo sarà mai, vedrete alcune obiezioni.

In particolare, ci sono due passaggi fondamentali.

### Fase 1: Acquisizione dell'Anatomia e Morfologia delle Singole Cellule

Il primo passaggio è l'acquisizione dell'anatomia e della morfologia delle singole cellule. Qui ci sono due figure che vi ho già introdotto l'anno scorso. Uno è Camillo Golgi, che ha inventato la tecnica dell'impregnazione d'argento (*silver staining*), di cui Corrado Calì ha parlato brevemente la settimana scorsa. Non sapevo, e l'ho imparato da Corrado Calì, che Camillo Golgi fosse di Pavia, quindi non lontano da Milano, Torino, ecc. Alla fine, non era particolarmente interessato alle conseguenze delle sue scoperte. Invece, Santiago Ramón y Cajal era un genio, come Golgi, per l'aspetto artistico e per la sua capacità di desumere la funzione dalla forma. E ci ha azzeccato, perché il fatto che una componente della morfologia dei neuroni, i dendriti, sia quella che riceve l'input, e l'assone e il soma siano quelli che portano l'output, è molto vicino a ciò che continuiamo a pensare oggi. Infatti, hanno ricevuto il Premio Nobel congiunto.

Una possibilità, non a mano come faceva Cajal guardando in un microscopio, è l'uso di una struttura interessante chiamata **camera lucida**. Oggi esiste un sistema digitale in vendita, credo da una o due compagnie nel mondo. Anticamente, c'era un particolare sistema di prismi che permetteva a Cajal e ad altri dopo di lui di vedere – forse la vostra generazione, essendo quasi subito digitale, ha dei traumi con queste cose – una cosa che permetteva di guardare all'interno di un microscopio, ma allo stesso tempo di vedere sovrapposta anche l'immagine della propria mano e di un foglio di carta posto sul tavolo davanti al microscopio. Questo permetteva, ovviamente a chi era bravo, di tracciare, non necessariamente osservando e poi riproducendo come un ritrattista, ma tracciando simultaneamente con la punta della matita, di un pennello o di qualsiasi altro strumento, nel punto esatto in cui questa proiezione, appunto questa camera lucida, mostrava l'immagine del microscopio.

Oggi è digitale, nel senso che c'è una telecamera ad alta definizione montata all'interno di un microscopio e l'immagine viene mostrata su un computer, sullo schermo di un computer. Con il mouse, o meglio con una penna ottica o qualche altra diavoleria che permette di puntare e indicare, una persona si mette e inizia a tracciare, cliccando con il mouse su questi punti. Clic, clic, clic, clic. Non solo, ovviamente, la persona ha il controllo del microscopio motorizzato.

Questa morfologia, ad esempio una cellula di Purkinje del cervelletto, è una cellula fantastica per la sua complessità spaziale, come potete rendervi conto. È particolare perché la sua morfologia è largamente su un piano. Se la vedeste di profilo, sarebbe estremamente sottile, come in un brevissimo romanzo di fantascienza chiamato *Flatland*, in cui esseri bidimensionali vivono in quel mondo. Questo, alla fine, è un neurone bidimensionale. In questo caso, non è necessario mettere a fuoco con il microscopio in modo eccessivo. Un po' sì, ovviamente, ma è un microscopio ottico, quindi non è una sezione ottica perfetta. In altri casi, avere un sistema motorizzato che permetta di spostarsi un po' su o un po' giù è molto comodo, perché quando l'operatore continua a cliccare, il sistema legge anche la coordinata, supponiamo Z, la coordinata verticale di un sistema di riferimento cartesiano. Quindi, automaticamente, quando clicca, clicca su punti che sono nel piano dell'immagine dopo aver messo a fuoco verso l'alto o verso il basso.

Per farla breve, con lo stesso spirito e lo stesso tipo o tipi diversi di impregnazione e colorazione dei neuroni, è possibile in un modo che oggi è quasi – come ci ha detto anche Corrado Calì – semi-automatico, perlomeno semi-automatico a livello ultrastrutturale con la microscopia elettronica, e a livello più mesoscopico (anche se è sempre microscopico, non ultrastrutturale), acquisire in larga misura, ma guidati da un computer o manualmente, le coordinate di tanti punti. Se questi punti vengono uniti da piccoli cilindri, è possibile ottenere digitalmente la ricostruzione digitale della morfologia di un neurone.

Badate bene che questa cosa che vedete qui, sulla base di tutta una serie di immagini unite (una specie di *collage*), è fatta perché il neurone è grande e, se si vuole una ricostruzione precisa, si deve usare un ingrandimento più elevato. Forse sapete che a maggiore ingrandimento la regione dello spazio dell'immagine visibile si restringe. Quindi, è necessario scattare più foto o acquisire più campi visivi, cioè regioni di interesse. Ovviamente, un computer è attaccato a un microscopio motorizzato che non solo ha l'asse Z – quindi l'obiettivo che va su e giù in modo riproducibile e che può fornire un *feedback* sulla profondità – ma anche un tavolino (*stage*) motorizzato, anch'esso con motorini. Oggigiorno, con le stampanti 3D, non è così impensabile trovare motorini piezoelettrici o altri meccanismi di posizionamento riproducibili e micrometrici. Si vuole muovere il piattino di pochissimo per avere un campo che magari è, per esempio, anziché qui, andare a questa regione di interesse e poi a questa. E si vuole farlo con riproducibilità, in modo che, unendoli, le coordinate abbiano un senso.

Tuttavia, se questo tracciamento digitale viene fatto con il software – mi ricordo il nome perché c'è una compagnia che lo vende, e credo sia stata acquisita da Nikon, Leica o Zeiss, una di queste – si chiama NeuroLucida. È, alla fine, un sistema di microscopio nemmeno particolarmente avanzato, un microscopio ottico in trasmissione, quindi la luce attraversa il campione. Non è particolarmente "figo", non ha necessariamente luce infrarossa, ecc., però è tutto motorizzato ed è attaccato a un computer dove c'è un software personalizzato. Tipicamente, studenti di dottorato con una pazienza enorme perdono, o meglio, investono settimane e settimane per ricostruire un neurone alla volta.

Quando il neurone è ricostruito, digitalizzato – se volete, come fare, non so se avete mai giocato con i pantografi, ma quella era ancora un'altra cosa, in cui avevate un sistema meccanico che permetteva di seguire un contorno e veniva ingrandito in qualche modo per l'aspetto meccanico – non è detto che non ci siano degli errori. Quindi, se questi punti, che qui dal punto di vista grafico sembrano eccellenti e ben connessi, dal punto di vista elettrico, se si vogliono collegare con piccoli segmenti di cavo (cilindri, assumendo che le proprietà geometriche del diametro siano costanti tra due punti – ovviamente, punto per punto l'operatore può indicare la larghezza che appare dall'immagine microscopica, ma questo potrebbe essere corretto o meno), non è detto che questi cavi siano poi elettricamente connessi l'uno all'altro. Quindi, se si inserisce direttamente questa morfologia in un simulatore, di cui parleremo credo oggi, e semplicemente lo si avvia, potrebbe non esserci nemmeno continuità spaziale.

Queste morfologie sono tipicamente "curate", nel senso che sono processate da persone o da algoritmi, in modo che siano utilizzabili non soltanto dal punto di vista estetico per una pubblicazione, non soltanto dal punto di vista morfometrico – perché una volta digitalizzato si potrebbero fare domande del tipo: "Quanti *branching*, quante biforcazioni partono dal soma?" oppure "Qual è la lunghezza media di un tratto di dendrite?". Questi sono, credo, dendriti apicali e basali di una cellula piramidale dell'ippocampo, CA1 o CA3. Si potrebbe chiedere: "Qual è la lunghezza prima di una biforcazione se il dendrite è basale?". In questo caso, potrebbe andar bene. Dal punto di vista della simulazione, però, deve esserci una continuità elettrica, altrimenti si avrebbe semplicemente una parte del neurone indipendente, con equazioni differenziali che non influenzano le altre, aggiungendo una complessità inutile.

Questa è la ricostruzione digitale, resa possibile grazie alla capacità di colorare le cellule. Nota: le cellule, una volta colorate, sono morte; è un tessuto fissato, non vitale, trattato con un *cocktail* chimico che lo ha ucciso. Tuttavia, è stato possibile sviluppare un qualche tipo di inchiostro che è visibile al microscopio.

### Fase 2: Misurazione delle Caratteristiche Elettriche e Identificazione dei Parametri

L'altro aspetto, di cui ovviamente abbiamo accennato e parlato nel corso dell'anno scorso, è non soltanto ricostruire la morfologia, ma misurare le caratteristiche elettriche. Anche qui, siamo ancora un po' nella preistoria, nel senso che se tutta la morfologia di un neurone è potenzialmente elettricamente attiva – e per "elettricamente attiva" intendo "elettricamente interessante" – è un cavo. Ho capito che possono esserci input sinaptici distanti, e poi il soma integra tutto quello che volete, però potrebbe anche essere che in punti diversi di questo cavo ci sia una distribuzione particolare di canali ionici. Quindi, potrebbe essere interessante poter vedere questi canali ionici. Non è facile; al massimo si potrebbe provare a colorarli con tecniche basate sull'immunologia, l'immunoistochimica, ma in quel caso non si avrebbe necessariamente un'indicazione quantitativa. Si saprebbe che sono lì, ma non quanti canali sono presenti.

Sto collaborando con un gruppo ungherese che sta osservando particolari neuroni della corteccia umana, qualcosa di analogo a quello che facciamo a Modena. In particolare, hanno trovato che alcuni neuroni – che non vi dico perché ancora non è pubblicato – potrebbero avere, cosa che non avviene nel ratto (e non sono neuroni piramidali, dove si sa che questo accade), nei dendriti, in particolare nel dendrite apicale (simile a questa immagine, sebbene questa sia un neurone piramidale di ratto), una distribuzione, pensiamo, addirittura non uniforme, di canali calcio voltaggio-dipendenti. Dall'immunoistochimica si vede la loro presenza, ma non quanti sono. L'altro passo che devono fare è utilizzare un microscopio a due fotoni, che è un microscopio speciale che produce una sezione ottica molto precisa, in condizioni vitali. L'obiettivo è vedere, con coloranti calcio-dipendenti, se per caso durante l'attività fisiologica di queste cellule – nonostante sia una fettina di cervello, mettere la testa del paziente sotto il microscopio potrebbe non essere così facile – si osserva un "puff" di calcio nei dendriti. Se così fosse, si potrebbe inferire indirettamente che lì devono esserci dei canali di calcio, perché quando c'è una qualche attivazione elettrica, si osserva un aumento improvviso di calcio intracellulare. Questo si può misurare e, conoscendo la conduttanza di un singolo canale (che si può determinare), si può desumere quanti canali sono presenti.

Questo per dirvi che, dal punto di vista della determinazione della funzione, il fatto che normalmente si utilizzi un unico elettrodo impiantato – non è propriamente impiantato, perché la tecnica del *patch clamp* non è esattamente intracellulare come gli *sharp electrode*, ma è come se lo fosse – solo nel soma, solamente da una parte, è come se io potessi farmi un'idea, un parallelo stupido, della bontà dei ristoranti della regione Emilia-Romagna, andando a mangiare in un solo ristorante di Mirandola. No, l'intera regione, l'intera morfologia del neurone, dovrebbe essere studiata.

È vero, *in silico*, se si crea un modello, si possono inserire quanti elettrodi si desidera, perché è sufficiente andare a guardare le variabili di stato che sono nella memoria del computer. Le ho definite io, posso andarle a guardare, posso decidere che durante la risoluzione numerica delle equazioni, di cui riparleremo o parleremo, semplicemente registro su disco il valore del potenziale di membrana o la concentrazione di calcio o quello che si vuole in un punto piuttosto che in un altro. Ma sperimentalmente, il meglio che si riesce a fare è mettere un elettrodo nel soma utilizzando microscopi tradizionali per la fisiologia, che sono di nuovo microscopi *upright* in cui l'obiettivo è sopra e la luce è sotto, e in trasmissione la luce passa attraverso il campione. Viene utilizzata una luce infrarossa perché permette di vedere le cellule. Se vedo le cellule, vedo perlomeno i bordi delle cellule; sembrano la superficie della luna, e c'è un interessante effetto tridimensionale che è una conseguenza di questa tecnica per aumentare il contrasto.

Posso, dato che vedo anche la punta della pipetta e la controllo con dei motorini, come in un videogioco, manipolarla letteralmente con un *joystick* o con altri attuatori, andare un po' più a destra, un po' più a sinistra, avanzare e penetrare la cellula. Se volete, una volta venite in laboratorio, vi faccio vedere questi micromanipolatori piezoelettrici, che costano 5-6 mila euro l'uno. Li vedete qui: questo è un *setup* in cui l'oggetto che vedete è un preamplificatore, sopra il quale è montato un *pipette holder*, il portatore della pipetta. È una cosa in cui si inserisce la pipetta di vetro, e questa pipetta viene chiusa, viene "pinzata". Nella parte che non si vede in nero, qui forse si vede di più, ci sono, oltre al preamplificatore, tre motori. Questi motori non sono ortogonali (XYZ), perché uno di questi non è ortogonale agli altri, ma va nella direzione della pipetta, che tipicamente è angolata, con una certa inclinazione per poter entrare, vedete qui, tra l'obiettivo e il campione. Se fosse ortogonale, non ci starebbe; deve essere obliqua.

Quindi, in teoria, potrei fare come Hodgkin e Huxley hanno fatto con l'assone gigante del calamaro – non con elettrodi di *patch clamp* o intracellulari – potrei avere prima la ricostruzione tridimensionale della morfologia. Ad esempio, nella pipetta posso avere un colorante; il colorante, dopo una decina di minuti, diffonde all'interno dell'intera morfologia della cellula. Quando finisco l'esperimento fisiologico, elettrofisiologico, tolgo la pipetta, mi resta la cellula, fisso tutto con paraformaldeide e altre sostanze, e aspetto di poter sviluppare con altre sostanze il colorante che ho lasciato intrappolato soltanto in quella cellula di cui ho la registrazione elettrica.

Tuttavia, non ho tantissimo tempo; le cellule a un certo punto muoiono. Quindi, nell'arco di 10, 15, 20 minuti, mezz'ora, se sono proprio fortunato, 40 minuti, massimo un'ora, riesco a fare un certo numero di protocolli in cui stimolo e vedo la risposta. Di nuovo, stimolo nel soma e vedo la risposta nel soma. Se siete proprio bravi, potreste evocare attività sinaptica nella periferia, con qualche stimolazione extracellulare di cui non parlo, e vedere l'effetto al soma. Oppure ancora, potreste avere un raggio laser che, per esempio con l'optogenetica o altre modalità (come faccio io adesso, qui non lo posso fare perché se premo una volta, è come se io puntassi un raggio laser qui, lo spostassi ripetutamente e dal soma vedessi elettricamente la risposta). Non posso fare tutto.

Si è parlato oggi con Silvestro Micera delle tecniche di identificazione dei sistemi dinamici; forse le avete sentite in qualche corso. Se si ha un sistema dinamico che è un impianto per la produzione di un bioreattore, una qualche produzione di un enzima o molecola chimica industriale, grande come un edificio, in cui si hanno un sacco di parametri, lì, se mi danno il tempo, ho tutto il tempo che voglio per fare ingresso-uscita. Posso magari iniettare il famoso impulso della risposta all'impulso; poi ci sono altri metodi, iniettare uno stimolo di controllo che può essere un processo stocastico e vedere la risposta. Nel caso di un sistema biologico, prima di tutto, non ho la possibilità di farlo in questo modo. In questo caso, sarebbe da farlo in tanti punti della morfologia del neurone e di farlo per il tempo necessario. Se devo farlo perché questo sistema è rumoroso, perché l'amplificatore lo vorrei ripetere magari 100 volte per poter fare le medie, per poter togliere una componente di rumore che non è correlata, potrebbe non essere accessibile.

Quello che però si fa, con un metodo di cui vi parlerò forse la settimana prossima, è dire: "Ok, ho messo insieme in un modello a questo punto molto complicato". Perché la morfologia... a un certo punto la domanda è: "Ma se ho tutto il dendrite apicale di questo neurone piramidale, lo posso considerare come un unico cavo?". Ho capito che il neurone è vero, quindi supponete che questo sia 300 micrometri e che questo diametro possa essere, supponiamo, un micrometro o anche meno. Lo posso considerare così oppure no? Lo devo spezzettare in più punti perché magari vedo che qui il diametro è un po' maggiore e poi man mano si assottiglia? Non so rispondere a tutte queste domande. Quello che però è sicuro è che a un certo punto dovrò scegliere una discretizzazione spaziale. Qui posso dire che c'è un unico potenziale $V$, lo chiamo $V_{\text{soma}}$ ed è un'unica variabile di stato. Qui, nella teoria di cui abbiamo beneficiato per inferire e comprendere i segnali extracellulari, in teoria ho $V_D$ come dendrite, o una funzione del punto e del tempo.

Però, nella vita reale, anche volendo risolvere l'equazione del cavo così, non lo so fare. Ho bisogno di discretizzare l'equazione continua alle derivate parziali sia nel tempo che nello spazio. Questo è un po' più *tricky*; magari non lo faccio io, lo fa un simulatore, però devo scegliere qual è il passo dove io penso che sia sufficientemente piccolo per cogliere qualunque tipo di cambiamento che potrebbe essere, per esempio, un segnale che nel tempo si propaga sui dendriti. Quindi, a un certo punto io devo fare delle scelte e ho una serie molto grande di parametri e di variabili di stato da identificare.

In particolare, queste ultime non le misuro. Quindi, come le vincolo? Io ho la registrazione in rosso di quello che è il potenziale al soma. Sì, ok, ho il modello, posso far andare la simulazione del modello e leggere soltanto il potenziale del soma, ma tutti gli altri li ho, li devo simulare. Non è che posso simulare soltanto un singolo compartimento, se la mia scelta è quella di dare importanza a questa dimensione morfologicamente complessa. Quindi, non è banalissimo, per farla breve, poter ottimizzare, adattare, trovare il *set*, l'insieme di parametri che possono essere numerosissimi, per esempio, che si adattano all'esperimento. Per esempio, qui lungo il dendrite, questo dendrite apicale, potrei avere che per ogni valore di $X$ io potrei avere un particolare valore di conduttanza massima dei canali sodio, che praticamente vuol dire "ci sono o non ci sono, quanti sono?". Quindi, questa è una funzione del punto. Potassio, calcio, ci sono altre conduttanze, quali sono? Esistono per caso dei gradienti in alcune conduttanze? Per esempio, nei neuroni piramidali, c'è un gradiente; in quelli piramidali ippocampali di cui vi ho fatto vedere, non è che la concentrazione, la densità, sia uniforme nello spazio, è maggiore in prossimità del soma, è minore in periferia, oppure il viceversa. Lo so, non lo so, sono tutti parametri liberi, e a seconda delle mie scelte io posso avere centinaia, se non migliaia di parametri da identificare.

Il *machine learning* e l'enfasi odierna sul *machine learning*, di cui ha menzionato anche lo *speaker* di questa mattina, dà l'impressione di dire: "Ma sì, è facile, ho un algoritmo, mi trova il minimo globale". *Good luck*, non è detto che sia così facile, non è detto che voi abbiate così tante misurazioni. Le avete su internet se dovete distinguere, classificare, quindi trovare i parametri di un modello come quello di una rete, alla fine un *multilayer perceptron*, una rete *deep*, che classifica se sono gattini o cagnolini, perché su internet c'è una quantità enorme, immensa di immagini. Già se fosse il particolare ritmo cardiaco legato a una malattia oppure no, non è che ci siano così tanti dati. E fare questa cosa del *big data* dipende. Sicuramente con gli *smartwatch*, nei prossimi anni ci sarà una quantità immane di registrazioni di dati fisiologici; tuttavia, chi li detiene? Apple, Samsung avranno questi *dataset*. Non credo che – o forse sì, quando tu lo compri ti dicono se vuoi, se accetti le condizioni lo puoi usare, quindi nelle condizioni c'è anche che loro possono, per studi clinici e quello che è, utilizzare i tuoi dati. Ma oggi, se io dovessi fare un qualche algoritmo che dice "adesso stai per avere...", non è che potrei contare su una quantità enorme di dati come in altri contesti dell'AI. I *large language model*, di nuovo, ok, è facile: prendi l'intera internet, qualche centinaio di terabyte, tutti i siti che puoi – a pagamento e non, leciti o non leciti, perché alcune cose sono *copyrighted* – tutti gli articoli scientifici che sono già in formato elettronico, tutti i libri che Google ha già scannerizzato anni fa, e che ovviamente non li... tutto questo è una quantità enorme di testo e lo posso mettere dentro un qualche aggeggio che può essere addestrato per predire la parola successiva o la lettera successiva.

### L'Approccio "Forza Bruta" all'Identificazione dei Parametri

Nel contesto della registrazione di dati, si potrebbe avere a disposizione un numero limitato di tracce, ad esempio 20 tracce registrate in mezz'ora, ciascuna lunga magari 5 secondi. Sebbene non sia un'impresa banale, negli ultimi anni si è osservato un rinnovato interesse per un approccio definito "forza bruta". Questo metodo mira a creare modelli la cui morfologia sia il più possibile simile a quella del neurone reale.

Per quanto riguarda le proprietà biofisiche, oltre alla distribuzione di densità, si considerano anche le proprietà cinetiche, come le costanti di tempo ($\tau$) o le funzioni di attivazione/inattivazione ($m_\infty$, $n_\infty$). Spesso, in mancanza di dati specifici per il neurone in questione, si ricorre a dati disponibili in letteratura, magari misurati su ratti negli anni '90 o 2000, assumendo che tali caratteristiche cinetiche siano conservate. Tuttavia, non vi è certezza che si tratti esattamente della stessa subunità o dello stesso canale ionico. Per superare questa incertezza e rendere il modello funzionale, si procede spesso ottimizzando un numero limitato di parametri, come la conduttanza massima.

Questo approccio ha avuto particolare successo grazie alla crescente potenza di calcolo disponibile. In passato, ad esempio, all'epoca della mia tesi di laurea, non era concepibile eseguire un'analisi dello spazio dei parametri così estesa per trovare un minimo. I computer non disponevano della potenza necessaria per esplorare in modo efficiente lo spazio delle soluzioni.

### Criteri di Aderenza Modello-Esperimento e Ottimizzazione

Una questione fondamentale è stabilire quando si può essere soddisfatti dell'aderenza tra modello ed esperimento. L'approccio comune prevede che sia il modello che l'esperimento ricevano lo stesso ingresso, tipicamente un impulso di corrente, come una forma d'onda quadrata di 200-300 millisecondi. Per le cellule in questione, l'iniezione di corrente potrebbe essere di 100-200 picoampere nel soma. Poiché queste cellule sono grandi, la resistenza di ingresso della membrana è relativamente bassa (dell'ordine di 20-50 M$\Omega$). Per indurre una deflessione del potenziale sufficiente a generare *spike*, sono necessarie decine o centinaia di picoampere. Questo sarebbe diverso se la resistenza di ingresso fosse più alta, come in cellule più piccole con poche conduttanze di *leak*, che si tradurrebbero in un'alta resistenza. In tal caso, un'iniezione di corrente di 20-30 picoampere causerebbe un aumento molto più significativo del potenziale di membrana.

Il processo di ottimizzazione consiste nel replicare sinteticamente l'esperimento con il modello e modificare i parametri fino a quando non si riproducono gli stessi osservabili. Gli osservabili possono variare: ad esempio, durante il mio primo post-dottorato, ho cercato di ottimizzare per il numero di *spike* e la frequenza, mirando ad approssimare la curva frequenza-corrente. La mia convinzione era, ed è tuttora, che a livello di rete ciò che conta sia la proprietà di trasferimento dell'informazione, piuttosto che l'esatta ampiezza dei potenziali d'azione. Tuttavia, i colleghi potrebbero obiettare che, se la forma del potenziale d'azione non è quasi identica a quella sperimentale, le proprietà biofisiche sottostanti potrebbero non permettere di prevedere correttamente la pendenza o l'attacco di questa curva frequenza-corrente. Quindi, si suggerisce la necessità di ottimizzare tutti i parametri contemporaneamente. Esistono, dunque, diverse filosofie per raggiungere questo risultato.

### Algoritmi di Ottimizzazione: Metodo del Gradiente vs. Algoritmi Genetici

Una tecnica di ottimizzazione importante, di cui si discuterà ulteriormente, è basata sugli algoritmi genetici. Il concetto di ottimizzazione o minimizzazione è familiare dall'analisi matematica, dove per trovare il minimo di una funzione si calcola la derivata e la si eguaglia a zero. Questo approccio, noto come metodo del gradiente, richiede la conoscenza della forma analitica della funzione costo.

Tuttavia, nel contesto della modellistica neuronale, la funzione costo, che è una funzione dei parametri liberi, spesso non ha una forma analitica nota. Ad esempio, una tipica funzione costo potrebbe essere la somma dei quadrati delle differenze tra il potenziale di membrana sperimentale ($V_{exp}(t_k)$) e quello del modello ($V_{mod}(t_k)$) in un certo intervallo temporale:
$$ C = \sum_k (V_{exp}(t_k) - V_{mod}(t_k))^2 $$
Questa funzione non può essere ottimizzata efficacemente con metodi basati sulla derivata prima uguale a zero.

In alternativa, gli algoritmi genetici adottano un approccio diverso. Anziché cercare il minimo tramite il gradiente, si prova casualmente una combinazione di parametri (un "genoma") e poi si introducono modifiche casuali (mutazioni genetiche) a questi parametri. Se una combinazione di parametri produce un valore inferiore della funzione costo, viene selezionata come "migliore". Questo processo iterativo porta all'evoluzione di generazioni successive di soluzioni, selezionando quelle con il costo minore. L'obiettivo è che la funzione costo sia il più vicino possibile a zero, essendo una quantità sempre positiva (somma di quadrati). In teoria, il minimo si ha quando tutti i termini, punto per punto nel tempo, sono minimi. In questo contesto, il *timing* preciso del potenziale d'azione può essere perso, indicando un compromesso.

### Ottimizzazione Multiobiettivo e Validazione del Modello

I risultati ottenuti negli ultimi dieci anni, anche grazie agli algoritmi di ottimizzazione, sono promettenti. Tuttavia, spesso non si mira a minimizzare la differenza punto per punto dell'intera forma d'onda, ma piuttosto a ottimizzare simultaneamente una serie di parametri osservabili, come l'ampiezza media degli *spike*, il numero di *spike* nel tempo (senza necessariamente il *timing* esatto), o altri criteri indipendenti. Questi criteri non vengono semplicemente sommati in un'unica funzione costo. Nel *machine learning*, tecniche simili, come la regolarizzazione, vengono utilizzate per imporre vincoli alla minimizzazione, ad esempio richiedendo che i parametri (come le conduttanze massime) non siano negativi. Tutto questo aggiunge complessità, ma esistono anche approcci più sofisticati, come la minimizzazione multiparametrica.

Dal punto di vista della valutazione, sorge la questione della riproducibilità. Se un esperimento venisse ripetuto più volte, si osserverebbe un certo *jitter* o sfasamento nel *timing* degli *spike*. Pertanto, una soluzione modellistica può essere considerata corretta se rientra nel *range* di variabilità che il neurone biologico stesso esibisce in ripetizioni successive. L'obiettivo non è replicare esattamente lo stesso identico comportamento, ma che il modello si situi nello stesso *range* di variabilità intrinseca biologica.

Tuttavia, questo approccio presenta delle insoddisfazioni. Richiede una notevole potenza di calcolo (supercalcolatori) e non garantisce che il modello trovato sia "il modello giusto", potendo rappresentare un minimo locale nello spazio dei parametri. Inoltre, è fondamentale valutare la generalizzabilità del modello. Se i parametri sono stati ottimizzati su un "training set" (un set di stimoli e risposte), è necessario verificare se il modello funziona altrettanto bene su un "test set" (una batteria di stimoli e risposte non utilizzata per l'addestramento).

Un'ulteriore limitazione è che un modello che funziona bene con stimolazione somatica potrebbe non essere rappresentativo delle proprietà del neurone quando l'input è sinaptico e distale. La registrazione da dendriti è tecnicamente molto più complessa rispetto alla registrazione dal soma, rendendo difficile la validazione di modelli dettagliati delle proprietà dendritiche. Questo porta a un certo pessimismo riguardo alla modellistica "forza bruta" che si propone di includere tutti i dettagli. Forse, dato che anche questi modelli sono intrinsecamente imperfetti, sarebbe più utile un approccio riduzionista che cerchi di estrarre solo l'essenziale.

## Costruzione di Modelli di Microcircuiti e Reti Neuronali

La strategia per la costruzione di microcircuiti, o addirittura di blocchetti di corteccia, ippocampo o cervelletto, si basa sull'integrazione di modelli con biofisica identificata elettrofisiologicamente e morfologie autentiche digitalizzate tramite tecniche di colorazione.

Il processo prevede innanzitutto il riconoscimento e l'implementazione della diversità morfologica. Ad esempio, nella corteccia esistono circa cinquanta diversi tipi cellulari. Digitalmente, si possono creare morfologie statisticamente simili e indistinguibili da quelle effettivamente registrate. Questo è necessario perché il numero di cellule registrabili è limitato (un centinaio di cellule in un database), mentre si parla di miliardi di cellule nel cervello. Per una simulazione realistica, non si possono semplicemente inserire tutte le cellule registrate, che provengono da animali o pazienti diversi. L'ideale sarebbe avere tutte le cellule di un singolo paziente o animale, il che è impossibile.

Piuttosto che assemblare cellule da animali diversi, si può assumere che la biofisica sia simile e utilizzare diverse soluzioni ottimali fornite dall'algoritmo di minimizzazione genetica. Le morfologie possono essere derivate con leggere variazioni rispetto a quelle registrate. Il razionale è che, sebbene non esistano due alberi esattamente identici, due alberi dello stesso tipo (es. due abeti) sono grosso modo simili. Tuttavia, questo "grosso modo" potrebbe essere il problema. La complessità dei dendriti potrebbe essere semplicemente un prodotto dell'evoluzione per catturare più input, analogamente alle ramificazioni degli alberi che massimizzano la cattura di luce. È anche vero che la crescita degli alberi è influenzata da fattori ambientali (es. evitare l'ombra di alberi vicini). Non è chiaro se la complessità dendritica sia solo un adattamento per la raccolta di input o se abbia anche proprietà computazionali intrinseche, ad esempio se una particolare distribuzione di canali ionici cambi l'eccitabilità del neurone. I modelli dovrebbero avere come missione quella di esplorare queste domande, non necessariamente di creare una mappa uno a uno della realtà. Potrebbe quindi essere utile semplificare: costruire una rete con tutti i dettagli e poi vedere quali dettagli possono essere eliminati, per identificare quali sono essenziali nel modello *in silico*.

Queste morfologie possono essere posizionate in uno spazio virtuale 3D tramite algoritmi di distribuzione. L'obiettivo è evitare che due neuroni occupino lo stesso spazio, approssimando l'enorme impaccamento dei neuroni osservato nella corteccia e in altri tessuti nervosi. Da questa disposizione 3D, alcuni ricercatori, come Henry Markham, hanno ipotizzato che la prossimità geometrica possa determinare i punti possibili per la formazione delle sinapsi. Questa ipotesi, chiamata "tabula rasa", suggerisce che la corteccia inizi con una molteplicità potenziale di contatti dove assoni e dendriti si trovano vicini; le sinapsi si formerebbero se necessarie, altrimenti verrebbero atrofizzate o rimosse. Non è certo che questo sia il caso, e non abbiamo sufficienti conoscenze per affermarlo con sicurezza.

La direzione è verso modelli che, sebbene "neuromorfi" nel concetto, potrebbero non riflettere esattamente la realtà. Il numero di sinapsi in queste distribuzioni 3D iniziali è enorme e viene successivamente "potato" (pruning). La giustificazione di ogni passo diventa più complessa man mano che il modello aumenta di complessità.

Dopo aver mimato le connessioni (che sono proposte statistiche piuttosto che ricostruzioni esatte), si introducono, cellula per cellula, i parametri biofisici che sono stati fittati con algoritmi di minimizzazione su singole cellule. Poiché non si hanno dati per tutte le decine di migliaia di neuroni "finti" in una simulazione (ottenuti variando leggermente la morfologia 3D), si può assumere che tutti i neuroni piramidali abbiano lo stesso set di parametri elettrici, o si possono utilizzare diverse soluzioni ottimali (es. due, tre, cinque) ottenute dall'algoritmo di ottimizzazione genetica per introdurre variabilità. L'importante è che questi parametri siano stati confrontati con l'approccio input-output simile all'esperimento.

Successivamente, si aggiungono proprietà dinamiche alle connessioni sinaptiche, come la depressione o la facilitazione a breve termine. Sebbene si disponga di dati per alcune combinazioni (es. piramidale-piramidale, piramidale-Martinotti, piramidale-basket cell), non si hanno dati per tutte le possibili interazioni (es. basket cell-Martinotti, cellule di strato 1 con piramidali di strato 2-3). In questi casi, si inseriscono i dati disponibili e si riempiono i vuoti con assunzioni, anche casuali, per poi avviare la simulazione.

Il valore indubbio di questo sforzo, che è più un'impresa di *software engineering* che di neuroscienze computazionali (ad esempio, trovare modi per manipolare rapidamente forme tridimensionali), risiede nella sua capacità di identificare le lacune nella nostra conoscenza. Permette di evidenziare quali esperimenti sarebbero necessari per determinare parametri attualmente sconosciuti, come l'efficacia sinaptica tra specifici tipi cellulari in diversi strati corticali. Inserendo valori arbitrari nel modello e annotando queste incertezze, si identificano i punti della conoscenza che richiedono ulteriore approfondimento.

## Sfide Computazionali e Modelli su Larga Scala

La simulazione dell'intero cervello non è ancora fattibile. Per simulare una singola colonna corticale, che contiene decine di migliaia di neuroni e milioni o miliardi di sinapsi, con ciascun neurone modellato con migliaia di compartimenti, è necessario un supercalcolatore o un cluster di High Performance Computing (HPC). Le architetture tradizionali di von Neumann stanno raggiungendo la saturazione, e la soluzione è aumentare il numero di microprocessori o core all'interno di ogni computer. Su una scala ancora più grande, un supercalcolatore può dedicare ogni core alla simulazione di un singolo neurone.

La simulazione in parallelo è cruciale, ma il parallelismo viene rallentato quando i neuroni devono comunicare tra loro. Sebbene i neuroni possano essere simulati in parallelo, l'accoppiamento è impulsivo e concentrato nel tempo (quando un neurone genera uno *spike*). In quel momento, è necessario che i neuroni post-sinaptici abbiano raggiunto lo stesso punto temporale nella loro simulazione. Il tempo potrebbe non essere sincronizzato, e l'obiettivo è che ogni processore lavori alla massima velocità, ma la comunicazione tra processori diventa un *bottleneck* nelle moderne architetture HPC. Quando i neuroni devono "parlarsi", la simulazione si blocca, le informazioni vengono comunicate, e il potenziale di membrana nel compartimento dove la sinapsi incide viene aggiornato. A causa di queste complessità, simulare pochi secondi di attività può richiedere un paio di giorni.

Il concetto di "digital twin" è molto attuale. Tuttavia, la simulazione di un intero cervello o dell'intera corteccia non viene realizzata con modelli multicompartimentali dettagliati, ma con modelli di popolazione. Questi modelli non rappresentano i singoli neuroni, ma intere popolazioni di neuroni, e spesso non includono nemmeno gli *spike* individuali. La variabile di stato non è il potenziale di membrana di ogni singolo neurone, ma un valore riassuntivo, come il *firing rate* dell'intera popolazione.

Questi modelli di popolazione si sono rivelati utili per fittare parametri da tecniche come l'elettroencefalografia (EEG) o la risonanza magnetica funzionale (fMRI), spesso utilizzate prima di interventi di neurochirurgia per rimuovere un *focus* epilettico. In questo contesto, si fittano i parametri non dei singoli neuroni o popolazioni, ma delle connessioni tra diverse popolazioni o aree cerebrali. Se un modello, seppur rozzo, riproduce ciò che si osserva con la fMRI, si può tentare di simulare digitalmente un'operazione chirurgica, rimuovendo alcuni nodi per vedere se la propagazione di attività patologica ultrasincrona migliora. In fase di *clinical trial*, questi modelli possono fungere da strumento di supporto decisionale per il neurochirurgo. La validazione clinica è difficile, poiché richiederebbe di seguire sempre le predizioni del modello e osservare gli effetti causali sul sistema biologico. La domanda è: se il modello fitta i dati e si modifica la sua topologia (e quella del sistema biologico), si osserva la stessa cosa?

L'approccio "forza bruta" è probabilmente molto lontano dalla fattibilità di simulare l'intera corteccia. Tuttavia, un vantaggio dei modelli dettagliati è la possibilità di esplorare gli effetti di interventi specifici. Ad esempio, se si introduce una molecola come le benzodiazepine, in un modello dettagliato si può specificare che queste cambiano la conduttanza massima di specifici recettori sinaptici, come i GABA-A. In un modello di *whole brain* basato su popolazioni, invece, non si ha il concetto di "recettore" in modo esplicito, e l'effetto della molecola sarebbe rappresentato in modo più astratto e meno diretto.

## Sfide Computazionali e Modelli su Larga Scala

### Il Concetto di "Digital Twin" e la Profondità di Modellazione

Il concetto di "digital twin" è estremamente affascinante e seducente. In teoria, la capacità di simulare un sistema complesso, come il motore di una Ferrari, prima della sua costruzione fisica, promette un notevole risparmio economico. Tuttavia, nella realtà, si impara che anche con materiali noti come l'acciaio, le proprietà sperimentali possono deviare dalle aspettative teoriche, ad esempio, nell'ottenere superfici estremamente levigate. Questo solleva una questione fondamentale per i sistemi biologici: perché non possiamo fare lo stesso con essi, e soprattutto, dove dovremmo fermarci nella descrizione?

Le equazioni di Hodgkin-Huxley, che sono state implicitamente suggerite e applicate a tantissimi compartimenti, rappresentano un livello di descrizione. Ma è sufficiente? O dovremmo invece utilizzare una descrizione stocastica e markoviana, in cui i singoli canali ionici sono descritti individualmente, tenendo conto delle loro caratteristiche di rumorosità?

La sinapsi, ad esempio, è stata approssimata come un semplice impulso di un millisecondo, un'onda quadra di un millimolare, per rappresentare la fusione delle vescicole. Ma è questa una descrizione adeguata? Quando dovremmo fermarci? Dovremmo includere l'equazione della diffusione per il neurotrasmettitore, o sistemi per il *reuptake*? Se si considerasse il punto di vista di Corrado Calì, si dovrebbe includere anche la dinamica di *buffering* ionico e neurotrasmettitoriale operata dalle cellule della glia. Ma se le cellule della glia non sono incluse nel modello, dove le si inserisce?

Spesso, non si dispone nemmeno della morfologia dettagliata delle sinapsi; sul computer, si ha semplicemente che il neurone 232 è collegato al neurone 1401. Anche se si modella l'assone, l'unica cosa che spesso viene inclusa in queste simulazioni è un ritardo, un $\Delta t$, come nel modello visto a lezione. Ma forse si dovrebbe modellare l'intero assone come un cavo eccitabile di tipo Hodgkin-Huxley, poiché sappiamo che possono verificarsi *failure* di conduzione, e il cervello, sia sano che malato, potrebbe sfruttare tali fenomeni.

Alla fine, non esiste un modo chiaro per sapere dove fermarsi nella complessità del modello. Pertanto, potrebbe essere più sensato utilizzare una descrizione semplificata, anche perché la connettività completa (la connettomica) non è ancora pienamente disponibile. Questo dibattito sconfina nella filosofia della modellazione.

### Limiti Attuali delle Simulazioni Numeriche Dettagliate

Un esempio di simulazione numerica con visualizzazione grafica, che possiede sia importanza scientifica che valore estetico, mostra una rete neuronale estremamente densa con numerose arborizzazioni e processi emergenti. Questa rete, spesso visualizzata in blu con occasionali cambiamenti di colore (più chiaro per la depolarizzazione), rappresenta l'attività di neuroni, principalmente piramidali ma anche di altri tipi cellulari, in un blocco di corteccia. Si osservano onde di depolarizzazione, indicando che la rete oscilla e spara su un arco temporale di qualche centinaio di millisecondi. Tali modelli, composti da neuroni semplici, possono essere eseguiti su dispositivi come cellulari o iPad, non richiedendo supercalcolatori per giorni.

Il valore di questi modelli dettagliati risiede nella possibilità di esplorare gli effetti di interventi specifici. Ad esempio, l'introduzione di una molecola come le benzodiazepine può essere esplicitamente modellata modificando la conduttanza massima di specifici recettori sinaptici, come i GABA-A. Tuttavia, sorgono limitazioni quando si considerano tecniche di stimolazione cerebrale più complesse, come quelle basate sugli ultrasuoni o la stimolazione elettrica.

Nei modelli attuali, non è possibile inserire direttamente la stimolazione elettrica come in un approccio agli elementi finiti (FEM, *Finite Element Modeling*). In un modello FEM, la membrana neuronale è vista come composta da blocchetti tridimensionali, ciascuno con proprietà dielettriche e di permeabilità elettromagnetica, dove le equazioni di Maxwell vengono applicate all'intero volume. Le simulazioni attuali, invece, spesso rappresentano i neuroni come una serie di cavi, ovvero un insieme di equazioni differenziali per un sistema monodimensionale (sebbene con biforcazioni e morfologia complessa). Non è possibile, quindi, modellare un elettrodo con elementi finiti, né gestire l'interfaccia metallo-elettrolita con le equazioni predefinite presenti nei programmi di simulazione multifisica. Questo rende complicato esplorare gli effetti meccanici, elettrici o ottici di tali interventi.

### La "Simulation Neuroscience" e il Progetto Human Brain: Critiche e Valore

Nonostante queste limitazioni, la modellazione dettagliata è considerata un approccio complementare, tanto che alcuni ricercatori hanno coniato l'espressione "simulation neuroscience" per definire un nuovo campo di ricerca. Tuttavia, a mio parere, non si tratta di un campo di ricerca nuovo, ma piuttosto di un'evoluzione dei modelli multicompartimentali che le neuroscienze computazionali hanno sempre utilizzato. Progetti come l'Human Brain Project, finanziato dalla Commissione Europea e dalla Confederazione Svizzera per oltre dieci anni, hanno permesso lo sviluppo di strumenti software e database, ma non hanno introdotto concetti fondamentalmente nuovi.

Questo tono critico è dovuto al fatto che l'Human Brain Project, in particolare, è stato oggetto di molte critiche, anche dal punto di vista scientifico, riguardo ai suoi obiettivi. Nella scienza, si cerca solitamente di falsificare delle ipotesi e si persegue un obiettivo specifico. L'approccio di "mettere tutto insieme, avviare la simulazione e vedere cosa succede" è legittimo, ma ha sollevato perplessità in molti.

Nonostante le critiche, un valore indubbio di questi progetti risiede nel fatto che, costruendo un modello con un approccio algoritmico, si possono identificare le lacune nei dati disponibili, poiché questi sono spesso sparsi. Questo permette di individuare i "buchi" nella conoscenza. In alcuni casi, in teoria, si potrebbe anche prevedere che, se certe oscillazioni fossero rilevanti (e non sappiamo se lo siano, dato che una fetta di corteccia di ratto in soluzione salina non oscilla sempre, a meno di condizioni molto specifiche come quelle osservate in pochi laboratori con fette di cervello di furetto non immerse ma all'interfaccia per un'ottima ossigenazione), la mancanza di parametri per le connessioni sinaptiche tra cellule di strato 1 e strato 5 potrebbe portare a inventare o adattare numeri per ottenere risultati fisiologici. In tal modo, la simulazione potrebbe fungere da ausilio nella predizione di dati biologici mancanti. Tuttavia, l'esperimento rimane, in teoria, la fonte primaria di verità.

Un articolo del 2015, insolitamente lungo, ha rappresentato la summa del primo sforzo di ricostruzione digitale di un microcircuito. Anche qui, ci sono due scuole di pensiero: c'è chi ritiene che non ci sia nulla di veramente nuovo, e che si tratti di uno sforzo computazionale che porta a un sistema molto complicato, che non comprendiamo più del sistema biologico stesso. Tuttavia, il valore predittivo, nell'ottica di identificare ciò che manca, è sicuramente valido.

### Limiti dei Modelli di Hodgkin-Huxley e Memoria a Lungo Termine nell'Eccitabilità Neuronale

Un'altra critica riguarda la sufficienza dell'approccio di Hodgkin-Huxley. Sebbene si possano aggiungere molte altre correnti ioniche oltre a quelle di sodio e potassio, ciascuna identificabile sperimentalmente con le proprie cinetiche, c'è chi sostiene che questo non sia sufficiente né rappresentativo dell'attività elettrica, e nemmeno di un singolo canale di membrana. I canali di membrana, spesso descritti come "aperti" o "chiusi", sono in realtà un'approssimazione. Si tratta di un modello cinetico markoviano che semplifica la complessa configurazione tridimensionale di una proteina.

Il modello AlphaFold di DeepMind, che ha ricevuto un notevole riconoscimento per aver trovato un metodo computazionale per prevedere la struttura terziaria delle proteine data la sequenza degli amminoacidi, dimostra la complessità intrinseca delle proteine. In questo contesto, l'assunzione di due stati discreti (aperto e chiuso, dove il poro è bloccato o aperto) è una semplificazione di un *continuum* di stati. Potrebbe non essere sufficiente, e si potrebbe pensare di aggiungere uno stato "inattivo" per il canale, come la famosa inattivazione mediata da una componente intracellulare, nota per alcuni canali. Tuttavia, potrebbe non essere sufficiente un numero finito di questi stati.

#### Esperimento di Simon Marom: Evidenza di Memoria Non-Markoviana

Un esperimento condotto dal collega Simon Marom, che ho tentato di invitare alla scuola estiva, ha rivelato aspetti sorprendenti. Utilizzando una matrice di microelettrodi su colture cellulari con una struttura di rete casuale, e bloccando la trasmissione sinaptica (rendendo le cellule chimicamente disaccoppiate), si è creato un sistema ideale per studiare l'eccitabilità di singole cellule isolate.

Stimolando casualmente un elettrodo, si può eccitare localmente un assone di un neurone vicino che passa in prossimità di un altro elettrodo. Questa stimolazione elettrica extracellulare genera un potenziale d'azione che si propaga sia ortodromicamente (verso la periferia, con scarso effetto data la disconnessione sinaptica) sia, in modo interessante e come noto dagli anni '40, in senso anterogrado, eccitando il soma del neurone a cui appartiene. Se un altro elettrodo è posizionato sotto il soma di questo neurone, si può registrare l'eco di questo *spike*.

Marom ha dimostrato che, stimolando in un punto, il potenziale d'azione si propaga all'indietro (*back-propagates*) e viene rilevato da un elettrodo vicino al soma del neurone eccitato. Questo permette di stabilire una relazione stimolo-risposta con una certa latenza.

Secondo il modello di Hodgkin-Huxley, se si ripetono gli stimoli a un tasso elevato, i canali del sodio si inattivano, e il sistema richiede un certo tempo (ad esempio, 5 millisecondi per il periodo refrattario assoluto) per resettarsi. Si potrebbe anche considerare l'adattamento della frequenza di *spike* (*spike frequency adaptation*) dovuto all'accumulo di calcio intracellulare e alle correnti di potassio calcio-dipendenti, che può estendere la refrattarietà relativa per centinaia di millisecondi. L'idea è che, se si attende un tempo sufficientemente lungo, il sistema dinamico dovrebbe rilassarsi e tornare a uno stato iniziale, o a un *steady state* di riposo (il potenziale di riposo, con tutte le variabili di stato $m, n, h$ che tornano ai loro valori di regime).

I biologi, tuttavia, sono meno sorpresi degli ingegneri da una visione più complessa, poiché la biologia è intrinsecamente disordinata. Esistono cascate di reazioni biochimiche, espressione genica e intercalazione di nuovi canali di membrana su scale temporali di minuti, ore o giorni. L'espressione genica, ad esempio, richiede diversi minuti, e le reazioni biochimiche sono interconnesse. Pertanto, non dovrebbe sorprendere che l'aspetto elettrico, sebbene mediato dai canali di membrana, sia solo un'eco o una manifestazione di una complessità biologica citoplasmatica, un "casino biochimico". Il neurone, in ultima analisi, non è un semplice circuito elettrico.

#### Fenomeni di Intermittenza e Transizioni di Fase

Questa complessità, anticipata dai biologi, è stata osservata sperimentalmente. Se si stimola un neurone e si registra la risposta (latenza), ripetendo l'esperimento una volta al secondo (1 Hz), si osserva che il sistema è riproducibile, indicando che ha avuto tempo sufficiente per tornare alle condizioni iniziali. Tuttavia, se si stimola leggermente più frequentemente, ad esempio a 20 Hz (ogni 50 millisecondi), i canali del sodio non sono pronti a rispondere. Inizialmente, la latenza tende ad aumentare, il che può essere spiegato dall'inattivazione parziale delle conduttanze del sodio, un fenomeno che si può riprodurre con il modello di Hodgkin-Huxley.

Tuttavia, dopo un certo periodo, quando la frequenza di stimolazione supera una "frequenza critica" (la cui natura critica non è ancora chiara), si verifica un fenomeno che i fisici chiamano "intermittenza", o che si potrebbe definire "comportamento caotico". Questo tipo di comportamento non è previsto dal modello di Hodgkin-Huxley. Un aspetto interessante è che, se si interrompe la stimolazione a 20 Hz e si torna a 1 Hz, il sistema recupera e torna a un comportamento deterministico.

Esperimenti prolungati per diverse ore o giorni (fino a 55 ore in alcuni casi) su colture neuronali sigillate e mantenute in incubatore, hanno mostrato che in alcuni momenti si manifesta questa intermittenza o caos. A un certo punto, in modo apparentemente improvviso, si verificano delle transizioni, come transizioni di fase (un concetto familiare ai fisici, come il passaggio dell'acqua da liquida a solida). Si osserva un cambiamento nel comportamento: lo stesso neurone che prima sparava in modo caotico (rispondendo a volte, ignorando altre, con latenze imprevedibili e molto maggiori), inizia a rispondere in "pacchetti". Ad esempio, risponde a uno stimolo, poi salta i successivi tre stimoli, e poi riprende a rispondere con regolarità, ma con periodi diversi.

Osservando questi esperimenti su scale temporali molto lunghe (ore), si nota che il sistema non è statico; compaiono strisce più scure che poi spariscono, per poi riapparire, suggerendo che il sistema possiede una sorta di memoria a lungo termine.

#### Implicazioni Matematiche: Processi con Legge di Potenza e Autocorrelazione

Dal punto di vista matematico, la situazione è ancora più complessa. Analizzando la serie numerica delle latenze o una sequenza binaria (0 per *failure*, 1 per risposta) con tecniche statistiche, si scopre che non esiste un unico tempo caratteristico che spieghi i *failure* dovuti all'inattivazione del sodio. Invece, si osserva una molteplicità di scale temporali, un processo che segue una "legge di potenza" (*power law*). Non c'è un'unica scala temporale dominante; tutte le scale temporali sono rappresentate.

Questo tipo di esperimento è stato replicato non solo con cellule neuronali, ma anche con oociti (cellule uovo, tipicamente di rana) in cui è stato introdotto, tramite tecniche di biologia molecolare, un singolo canale ionico (ad esempio, un canale sodio o potassio). Anche in questo sistema semplificato, si osserva lo stesso fenomeno. Ciò suggerisce che questa proprietà, ovvero l'esistenza di molteplici scale temporali, è probabilmente universale a livello delle singole molecole, quando queste sono organizzate in una cellula, e quando le cellule sono organizzate in una rete. Il gruppo di Shimon Marom ha dimostrato la stessa cosa: la molteplicità di scale temporali esiste a livello di singolo canale, di singola cellula e di rete.

Shimon Marom ha descritto questo fenomeno come se all'interno del sistema biologico (canale, cellula o rete) ci fossero "tanti orologi". Quando si esegue un esperimento di breve durata (pochi secondi, trenta minuti per una *slice*, o qualche ora per un esperimento comportamentale), si osservano solo gli "orologi" con periodi corrispondenti a quella scala temporale, mentre gli altri, pur non essendo visibili, influenzano la risposta e le sue proprietà.

Sembra quindi che l'eccitabilità, sia a livello cellulare che sinaptico (come dimostrato in un articolo congiunto), possieda una memoria intrinseca. Il processo generativo di queste sequenze binarie (successo/fallimento della risposta sinaptica o della risposta di rete a una stimolazione) non è semplice.

Tecnicamente, uno dei metodi di analisi è il calcolo della funzione di autocorrelazione. Per un processo stocastico $X(t)$, la funzione di autocorrelazione, che dipende dal ritardo $\tau$ con cui le realizzazioni sono confrontate con se stesse, è tipicamente una delta di Dirac centrata in zero per un processo gaussiano bianco. Ciò implica che la correlazione tra due punti successivi del processo, per quanto vicini, è zero. Se il processo è gaussiano, questo significa che i punti sono statisticamente indipendenti. Tuttavia, i processi biologici reali non sono processi bianchi; la loro funzione di autocorrelazione non è un singolo esponenziale, ma una combinazione di molti esponenziali, seguendo una legge di potenza. Questo comportamento non è catturato dal modello di Hodgkin-Huxley, che è stato identificato su esperimenti di poche decine o centinaia di millisecondi.

## Implicazioni Matematiche: Processi con Legge di Potenza e Autocorrelazione

### Limiti Temporali nell'Identificazione dei Modelli Neuronali

Axel e Hodgkin hanno identificato le cinetiche dei canali ionici applicando impulsi in *voltage clamp* della durata di qualche decina di millisecondi. I modelli sviluppati da Markram e colleghi, noti come "modelli pelosi" per la loro complessità, si basano su registrazioni di *spike train* ottenute stimolando per centinaia di millisecondi o anche qualche secondo. Attualmente, non esiste un metodo consolidato per descrivere questi processi su scale temporali estese, sebbene ci siano tentativi di approssimazione. Non è chiaro se sia necessario un numero infinito di costanti di tempo, ma certamente un numero elevato. [Inference] Le equazioni per catturare tale complessità non sono ancora ben definite. Questo suggerisce che un modello di quel tipo è valido solo per la scala temporale utilizzata per identificarlo. Non è possibile "stirarlo" per farlo funzionare per secondi, decine di secondi o minuti, poiché non sarebbe rappresentativo.

Anni fa, durante una delle prime conferenze dell'American Society for Neuroscience, la più grande società di neuroscienze al mondo, Larry Abbott, un pioniere nel campo della neuroscienza computazionale, ha sottolineato come in neuroscienze si tenda a creare "fettine" non solo di tessuto, ma anche di scale di organizzazione e scale temporali. Gli esperimenti durano pochi secondi, millisecondi o centinaia di millisecondi, e il sistema risponde effettivamente su quella scala temporale con cui viene stimolato. Questo è un comportamento tipico dei sistemi con risposta a legge di potenza. Inoltre, si creano "fettine" anche tagliando la dimensione spaziale, ad esempio utilizzando sezioni di tessuto di 300 micrometri. Tutto ciò implica che la comprensione finale è condizionata dalla specifica scelta sperimentale. Non esiste un esperimento "corretto" che permetta di osservare tutto. Alcuni esperimenti, in teoria, non dovrebbero essere estesi a tempi per i quali non sono stati progettati. È quindi inutile, perché si vorrebbe un modello che possa evolvere nel tempo e magari esprimere un fenotipo di neurodegenerazione. Non è impossibile che il concetto di "digital twin" possa realizzare ciò. Tuttavia, è importante considerare che ciò potrebbe non essere possibile. L'approccio di Hodgkin-Huxley e il tipico esperimento *in vitro*, in cui si pensa di poter identificare i parametri del modello da un punto di vista riduzionista, non sono sufficienti. Quei parametri sono probabilmente validi solo per quella scala temporale, a meno che non si riescano a identificare i parametri su scale temporali che gli esperimenti convenzionali non coprono. Sarebbe necessario condurre esperimenti molto più lunghi, con protocolli di stimolazione che non durano 10 millisecondi, ma centinaia, migliaia di millisecondi o persino un'ora. Sebbene si sappia che dopo qualche minuto si può perdere la cellula, questo sarebbe l'approccio necessario.

### Oltre il Genoma: L'Evoluzione degli Approcci "Omici"

Avete mai sentito parlare di genoma e genomica? Qualche anno fa, specialmente negli anni '80 e '90, c'era l'idea che comprendendo e sequenziando il genoma si potesse sapere tutto. Fino a pochi anni fa, e forse ancora oggi, esistono servizi commerciali che, tramite un campione di saliva, offrono un'analisi del genoma. Un'azienda, 23andMe (che credo sia fallita), forniva informazioni sul *background* etnico (es. "sei vichingo" o "arabo, con un po' di Sicilia, Puglia, ecc."). Altri servizi identificavano mutazioni che potevano essere predittive di malattie.

Oggi sappiamo che il genoma da solo non è sufficiente. Lo slogan "io sono il mio genoma" non funziona più. Il genoma è come un'impalcatura, ma ciò che conta è quali geni e come vengono espressi, ovvero il profilo di espressione genica. Probabilmente avete una conoscenza più approfondita di biologia molecolare. Il proteoma, di cui avrete sentito parlare, è un aspetto successivo che mira a comprendere il *pattern*, il profilo di espressione proteica. Anche questo, probabilmente, non è sufficiente. Esiste un ulteriore campo di studi, menzionato da Corrado Calì la settimana scorsa, che si ispira a questi approcci "omici", basati su tecniche ad altissimo *throughput*. La possibilità di sequenziare il genoma significa derivare una quantità enorme di informazioni. Credo, se non ricordo male, che nel caso del genoma, un ricercatore stampò una pila altissima di pagine A4, alta diversi metri, per rappresentare l'intero genoma.

### La Connettomica: Promesse e Limiti

C'è un'altra area di studio, introdotta da Corrado Calì, nota come connettomica. L'idea è che "io sono la persona che sono perché i miei neuroni hanno un particolare schema di connettività". Questo schema di connettività sarebbe l'ingrediente mancante, ad esempio, nei modelli complessi ("pelosi") ma anche nei modelli più semplici, dove i singoli neuroni potrebbero essere molto basilari, magari singoli compartimenti (ignorando la loro estensione morfologica spaziale). L'importante sarebbe inserire le connettività corrette, che potrebbero essere identificate tramite una serie di esperimenti.

Corrado Calì ha citato, e io ero presente, un congresso della Society for Neuroscience dove Sebastian Seung, una figura di spicco (prima al MIT, ora credo a Princeton), ha tenuto un *talk* estremamente accessibile. È una persona molto carismatica, con una dialettica e un linguaggio del corpo particolari e gradevoli. In modo provocatorio, ha ripetuto con enfasi durante tutto il suo intervento: "Io sono il mio connettoma". Egli sogna un futuro, probabilmente realizzabile ma non senza problemi e non così scontato che sia utile, in cui le malattie verranno diagnosticate come malattie del connettoma. In alcuni casi, come l'autismo e la schizofrenia, si sa già che sono malattie del connettoma. Seung afferma: "I am my connectome, non sono un genoma, sono il mio connettoma". Se si riuscisse a comprenderlo, misurarlo e descriverlo, si potrebbe in teoria ricostruire una persona. Non proprio, anche a causa delle scale temporali e dell'immensa complessità dei processi biochimici che si dovrebbero riattivare, di cui non si hanno le condizioni iniziali, ma questa è un'altra storia.

Seung ha scritto un libro divulgativo, "Connectome" (credo sia stato tradotto in italiano), dove nelle prime pagine paragona gli spermatozoi e i neuroni come gli estremi più affascinanti di un minimalismo biologico estremo. Da un lato, la semplicità di nuotare e fecondare l'ovulo; dall'altro, una complessità barocca, la più complicata possibile, per raccogliere *input* e svolgere una funzione estremamente differenziata. Questa descrizione, in particolare per introdurre il sistema nervoso, mi ha molto colpito. È un ottimo divulgatore; potete guardare il suo TED Talk, che dura circa 20-30 minuti. Il *talk* alla Society for Neuroscience era più orientato a un pubblico di neuroscienziati.

### Critica alla Connettomica: L'Esempio di *C. elegans*

La mia critica è che già dagli anni '80 esistono diagrammi di connettività per organismi molto semplici come il nematode *C. elegans*. Questo piccolo invertebrato ha esattamente 302 neuroni. Negli anni '80, gli sforzi di un ricercatore (Sydney Brenner, Premio Nobel per scoperte in biologia molecolare) riuscirono a catalizzare un enorme sforzo tecnologico per l'epoca, simile a quello descritto da Corrado Calì (sezioni di *imaging* con microscopio elettronico a scansione, simultaneo al taglio). In quel caso, la tecnologia non era così avanzata; si trattava semplicemente di *imaging* con microscopia elettronica a trasmissione. Con pazienza certosina, furono allineate migliaia e migliaia, se non milioni, di fettine per descrivere come questi 302 neuroni sono collegati tra loro tramite circa 7000 sinapsi, che sono sempre le stesse.

Oggi, nonostante abbiamo il diagramma di funzionamento di *C. elegans*, non abbiamo dei *C. elegans* che navigano nello spazio. Anche se i famosi aspirapolvere Roomba evitano gli ostacoli e forse puliscono (ne ho uno a casa, di un'altra marca cinese, che "funzionicchia"), non funzionano così. Nonostante abbiamo imparato molte cose, non siamo riusciti a ricostruirlo. La gente ha capito che non è così banale; non basta sapere come due neuroni sono collegati. In teoria, manca l'aspetto della neuromodulazione.

### Il Ruolo Cruciale della Neuromodulazione

Nel cervello e in qualsiasi sistema, anche negli invertebrati come l'aragosta o il granchio (di cui non ricordo il nome specifico in questo momento), esiste una potentissima e variabilissima presenza di neuromodulatori. Questi non agiscono tra il bottone sinaptico e il dendrite, ma come un segnale di *broadcast*, di diffusione su scale spaziali molto estese, che possono riconfigurare il circuito in un modo o nell'altro.

C'è una ricercatrice, una scienziata molto brillante a Boston, Eve Marder, che ha dedicato la sua vita a studiare questo fenomeno. È una figura molto illuminata, nel senso che se abbiamo una combinazione di interessi tra fisici, elettrofisiologi e neuroscienziati, lo dobbiamo a lei, che ha bussato alla porta di Larry Abbott (che si occupava di astrofisica, quindi non c'entrava direttamente) e ha iniziato a proporre una collaborazione per sviluppare modelli insieme.

Sperimentalmente, Eve Marder ha studiato le proprietà di un circuito, il nucleo pilorico, coinvolto nella digestione di un granchio. Questo circuito si contrae e produce un'oscillazione. Ha osservato, e in anni recenti ha sistematizzato e pubblicato in modo molto convincente, che a seconda della stagione e della temperatura dell'acqua (es. 5 gradi o 20 gradi nella baia di Boston), l'animale funziona allo stesso modo. Mentre un sistema del genere, se simulato al calcolatore, cambierebbe le cinetiche dei canali con la temperatura e la simulazione produrrebbe un comportamento completamente diverso, l'animale fa la stessa cosa.

Inoltre, Eve Marder, insieme ad altri ricercatori, ha dimostrato che due neuroni che svolgono esattamente la stessa funzione, e sono quindi indistinguibili dal punto di vista dell'output (*pattern*), possono avere un profilo di distribuzione di canali ionici completamente diverso quando studiati nella loro composizione. Ad esempio, uno può avere molti canali del sodio e pochi del potassio, mentre l'altro può avere pochi canali del sodio e molti del potassio. Nonostante ciò, il risultato elettrico di risposta è identico.

Questo sembrerebbe suggerire che, se si vuole costruire un modello, è estremamente difficile, perché trovare il giusto set di parametri in natura è un problema sovradimensionato. Esistono minimi locali nella potenziale funzione di costo da minimizzare, e ci sono soluzioni multiple che danno lo stesso risultato. Queste soluzioni esistono in natura, quindi non si può scartarne una perché "irrealistica" (a meno che non si abbiano conduttanze negative, che non hanno senso). Poiché è un sistema complesso con molti parametri, e sembra che la stessa risposta sia ottenuta in modi diversi, questo conferisce agli animali e agli esseri viventi una notevole robustezza, ma rappresenta un grosso grattacapo per i modellisti: quali valori inserire? I valori identificati in precedenza in quel modello sembrano corretti e si adattano al *training set*, ma potrebbero non corrispondere esattamente alla combinazione di parametri di quel neurone specifico. Si potrebbe obiettare che se un animale li usa in modo intercambiabile, anche una simulazione potrebbe farlo. Forse, non lo so.

### Ulteriori Limiti della Connettomica

Quindi, il connettoma non è sufficiente. Ci sono anche altri problemi: è un'istantanea a livello di microscopia elettronica ultrastrutturale, valida solo in un preciso istante. Se si congelasse il mio cervello, sarebbe un'istantanea, ma io sono un processo, e probabilmente le mie connessioni sono estremamente plastiche. Comprendo che nel corpo calloso ci siano sempre le stesse fibre. Corrado Calì forse ha menzionato la settimana scorsa che ogni volta che si prepara e si apre il cervello di un animale, è sorprendente quanto sia sempre più o meno uguale. L'ippocampo, il corpo calloso, la corteccia appaiono sempre simili. Ma su una microscala, le cose possono non essere le stesse e cambiano nel tempo. Silvestro direbbe che non necessariamente cambiano per la plasticità cerebrale decantata da tutti, ma cambiano. E quindi, a cosa serve uno *snapshot*?

Wilfried Denk, uno dei co-inventori di questa tecnica o campo di ricerca della connettomica (tedesco e purtroppo parkinsoniano), ha detto, e me lo ricordo perché l'ha detto ripetutamente, anche a me in un convegno che avevo organizzato in Belgio: "Ora che hai questo diagramma, cosa fai? Lo dai ai teorici? Perché i teorici lo mettano in un modello?". E lui ha risposto: "No, potrebbe non avere senso. L'unica cosa che potrebbe avere senso è analizzare il connettoma dal punto di vista statistico". Quindi anche lui non è così convinto dell'idea di avere il diagramma di connettività. Se avessi il diagramma circuitale di questo telefono, in teoria potrei ricostruirlo e dovrebbe funzionare allo stesso modo (a parte il software). Ma se non fosse un dispositivo programmabile, se fosse un dispositivo analogico, un amplificatore che produce musica, se avessi il diagramma circuitale e gli stessi componenti, potrei costruirlo a casa. Anni fa, gli appassionati di elettronica compravano kit: dovevano saldare i componenti su un circuito stampato e poi funzionava. Forse nel cervello non è così. Però, almeno si potrebbe dire se quello è un cervello di un ratto con Alzheimer o il connettoma di una piccolissima frazione di corteccia di un malato di Alzheimer o Parkinson. Forse si possono vedere a livello statistico quali sono le differenze, certamente non microscopiche o puntuali, perché alla fine si tratta di ricreare dal punto di vista statistico e morfologico una connettività forse tipica.

Inoltre, si esegue su un singolo animale. Attualmente, il record è un millimetro cubo, se non ricordo male, della corteccia visiva di un ratto o di un topo, probabilmente un topo. Si vorrebbe farlo su almeno 100 o 1000 topi. Credo che a livello di microscala possa essere molto diverso dal genoma. Magari il genoma di due topi che appartengono allo stesso *background* genetico potrebbe essere simile, ma non esattamente identico. Il connettoma sicuramente non è identico.

Ma soprattutto, il problema che ho menzionato è la neuromodulazione. Non la conosciamo, non la sappiamo, e può alterare drammaticamente la connettività. Potrebbe essere una specie di bacchetta magica che dice: "Questo circuito cambia, diventa completamente diverso, si riassembla. Queste innovazioni vengono depresse, queste vengono potenziate". Semplicemente quando si rilascia dopamina, adrenalina, acetilcolina, o altri neuromodulatori ad ampio raggio. E quindi, in teoria, la cosa interessante potrebbe essere estrarre da questa ridondanza, che sicuramente c'è, dei principi statistici, non una replica dettagliata. E se è qualcosa di qualitativo, qualcosa di statistico, allora perché fare un modello di forza bruta?

### Differenze tra Neuroni Umani e Roditori

Prima di fare un'altra pausa, forse ve l'ho già raccontato l'anno scorso, o forse qualcuno di voi mi ha seguito al Caffè Scienza a Modena. La maggior parte dei dati che ho sono per i roditori, in particolare. Non ho dati specifici per i primati e per gli uomini. Posso traslarli sugli uomini? Ho così tanta fiducia nel fatto che ricostruendo un frammento di corpo in un supercalcolatore, si impari qualcosa e si possa creare un "digital twin" del cervello umano?

Abbiamo descritto la cosa, avendo dei pezzi di cervello umano che ora stiamo continuando a ricevere. Doveva essere lunedì, ma sarà probabilmente martedì, una chirurgia a Baggiovara in cui il paziente ha dato il consenso informato e ha accettato. Ci verrà dato un pezzettino di corteccia. Qualche anno fa, in uno studio pionieristico a cui ho avuto la fortuna e l'onore di partecipare e co-dirigere, abbiamo osservato che i neuroni umani, prima di tutto dal punto di vista morfologico, sono diversi: sono molto più grandi. E qui potreste dire: "Vabbè, lo stesso tipo di digitalizzazione la puoi fare anche qui". Bene.

Dal punto di vista elettrico, forse vi ricorderete (ve l'ho raccontato o forse no), le proprietà del potenziale d'azione dell'uomo, anche se vi ho detto l'anno scorso che se siete scimmie, gatti, rane, nematodi, vermi, il potenziale è sempre quello. Quindi l'informazione, la diversità (se sono io, un cane, un gatto, un ratto o altro, perché cani e gatti sono molto intelligenti), deve risiedere nella frequenza, nel codice temporale, nella connettività, in quello che è. Però sembra che nell'uomo ci siano diversità nella ripidità del potenziale d'azione. Banalmente, è probabile che a livello di subunità dei canali del sodio voltaggio-dipendenti (NaV1) ci sia una cinetica diversa. Ho iniziato ad avere questi dati, ma non ne ho così tanti.

Ho trovato una cellula, una cellula piramidale dello strato 2-3 della corteccia temporale di un paziente (sia tumorale che epilettico), che ha potenziali d'azione più ripidi di quelli del ratto. E non è solo un fatto estetico. Se si guarda un potenziale d'azione isolato, non sono distinguibili da quelli del ratto o del topo. Ma nel caso di un *firing* continuo, è come se quelli del ratto fossero più lenti, si stancano subito. È come se l'inattivazione fosse meno evidente per l'uomo. Non ho così tanti dati; dovrei rimettere in discussione tutto ciò che ho imparato dai roditori.

C'è un'altra differenza che non vi mostro, in cui anche le sinapsi tra quel tipo cellulare (sinapsi tra cellule piramidali dello strato 2-3) hanno una depressione sinaptica diversa. Se vi ricordate, l'anno scorso vi avevo parlato di questo modello di Zodix-Markram, che può essere identificato (e l'abbiamo identificato su quelle tracce). Uno dei parametri era una costante di tempo di recupero dalla depressione sinaptica. Le cellule umane si recuperano prima. Se si dà un treno rapido, le sinapsi di ratto e topo, avendo un valore più elevato per questa costante, tendono in qualche modo a non rispondere. Rispondono un po', ma non c'è più neurotrasmettitore nel bottone sinaptico. Quelle dell'uomo sono "up and running", hanno una capacità, forse ho fatto un esempio sgradevole, di dire "mi dovete chiedere di sputare un lama". Forse è più elegante dire che, anche se la frequenza di questi impulsi, anche ravvicinati, è molto elevata, il lama riesce a rigenerare, a ripristinare le sue risorse di neurotrasmissione per sputare più rapidamente perché questo valore è più basso.

E quindi, in teoria, ci sono differenze fondamentali tra il topo, il roditore e l'uomo. Spero di poter fare quello che abbiamo fatto in 50-100 anni di ricerca sui modelli sperimentali animali, ma oggi non ho il gemello digitale.

### Il Paradosso della Mappa 1:1 e la Scienza

Volevo concludere questa parte dicendo che l'approccio attuale che ho criticato è forse simile, anche se in modo ottimistico, a quello di avere una mappa uno a uno. In un racconto di fantascienza dello scrittore spagnolo Borges, si parla di una mappa uno a uno, ovvero una mappa che, come quella di Mirandola, è grande quanto lo spazio che descrive. Il racconto narra che queste mappe, in futuro, non soddisfacevano più l'uso delle persone. Le mappe convenzionali e i cartografi decisero di creare mappe dell'impero la cui dimensione era quella dell'impero stesso e che coincidevano con l'impero punto per punto. Tuttavia, erano molto complicate da srotolare, estremamente ingombranti. Nelle generazioni successive, le abbandonarono perché non erano pratiche, erano inutili. E così, nei deserti dell'Ovest, ancora oggi ci sono rovine, resti di questa mappa abitata da animali, mendicanti e accattoni. "Tutte le terre, there is no other relic", e non c'è più alcun relitto, alcun residuo della cartografia.

Quindi la domanda è: è utile avere un modello uno a uno in cui si inserisce tutta la complessità? Ma la scienza non dovrebbe essere qualcosa di parsimonioso, facile da falsificare? Non ho qui una *slide* in cui c'era il bacio di Klimt, che forse qualcuno di voi ricorda, una rappresentazione di due amanti che si baciano, estremamente complicata, ricca, barocca. E poi il bacio di uno scultore, credo rumeno, che è estremamente semplificata. Quale è più utile? Quale rappresenta il fenomeno fisico? Questo bacio di Brancusi o il bacio di Klimt? A mio avviso Brancusi, anche se esteticamente Klimt è molto più bello da vedere.

Ci fermiamo 10 minuti. Sono le 15:55, perdonate il tempo extra, quindi dobbiamo fermarci alle 16:45. Quindi è un'altra mezz'ora, perché adesso vi faccio fare 15 minuti di sosta, giusto? Ok, se no ditemi voi, non voglio abusare del tempo. Ecco, che giusto è giusto.

### Introduzione all'Approccio Basato sulle Conduttanze (Modelli HH Multicompartimentali)

Una tematica di cui voglio parlarvi, sebbene in un contesto di un gruppo così piccolo, è la possibilità di espandere gli argomenti che possono essere di vostro interesse. Ho un programma che intendo svolgere, ma possiamo deviare o espandere alcune parti, o rendere queste lezioni più interattive nel caso in cui vogliate avere un'attività ancora più pratica rispetto a quella che vi mostro. Questo è in particolare uno degli approcci, l'approccio, se volete, di forza bruta. Lo inquadro in un contesto in cui per il momento vi mostro come si simula un singolo neurone con questo approccio basato sulle conduttanze, si dice, tipo Hodgkin-Huxley come concetto, ma anche di modelli che possono essere multicompartimentali.

L'aspetto centrale di questo approccio è che esista una complessità inerente nel cervello che vale la pena di essere catturata, particolarmente legata alla morfologia e al fatto che ci siano delle conduttanze voltaggio-dipendenti oppure concentrazione-dipendenti. Potrebbe quindi non essere così utile semplificare e scartare dettagli che sperimentalmente osserviamo.

Questi sono dei numeri solo per impressionarvi: ci sono $10^{11}$ neuroni probabilmente in un cervello di primate umano, nell'uomo. Ci sono $10^{15}$ connessioni, e ogni neurone riceve $10.000$ *input* o anche $100.000$ nel caso di un neurone umano da altri neuroni, in particolare della corteccia. Questo è un numero che mi impressiona perché ha a che fare con il numero di *input* che un neurone piramidale tipico, come quello che disegno sempre, riceve.

## Introduzione all'Approccio Basato sulle Conduttanze (Modelli HH Multicompartimentali)

### Complessità e Diversità Neuronale

Si tratta quindi di un'enorme complessità, sia a livello quantitativo, in termini di numeri, sia a livello di connettività. È, di nuovo, alquanto complicato e ambizioso pensare di riuscire a mappare il connettoma su scala dell'intero tessuto corticale, per non parlare dell'intero cervello, considerando che un singolo neurone può avere centomila altri interlocutori. Inoltre, esiste una notevole diversità tra i diversi tipi di neuroni, una diversità che nei modelli semplificati viene spesso trascurata. Non mi riferisco solo alla distinzione tra tipi cellulari eccitatori e inibitori, ma anche a diverse proprietà e all'espressione di specifici *marker* molecolari.

Esistono neuroni inibitori, in particolare nella corteccia, che presentano una varietà dell'ordine di 20-30 tipi distinti. Questa diversità di 20-30 tipi implica, per cominciare, differenze nella loro morfologia, con una specificità legata a caratteristiche morfogenetiche. Ad esempio, alcuni hanno dendriti che si estendono fino ai *layer* 1-2, mentre altri ricevono *input* da altri *layer*. Alcuni hanno assoni che proiettano al di fuori della corteccia, altri ancora proiettano a un particolare *layer* o strato della corteccia. A ciò si deve aggiungere la varietà di tipi funzionali.

Esistono neuroni inibitori che mostrano un comportamento cosiddetto *fast spiking*. Se si applica un impulso di corrente, ad esempio di 50-100 pA, questi neuroni iniziano a scaricare potenziali d'azione in modo estremamente rapido, raggiungendo una frequenza di scarica dell'ordine di 80 *spike* al secondo. Esistono però altri neuroni, anch'essi inibitori e quindi rilascianti GABA, la cui attività è di tipo diverso: mostrano un fenomeno di accomodamento, e la loro frequenza di scarica a regime è dell'ordine di 10-20 *spike* al secondo. Durante il transitorio iniziale possono raggiungere frequenze più elevate, ma questo transitorio dura solo poche decine di millisecondi.

Ad esempio, i neuroni *fast spiking* (uno dei tipi a cui mi riferisco) esprimono un *buffer* del calcio chiamato parvalbumina, mentre altri tipi, come quelli con accomodamento, esprimono la somatostatina. Questi sono *marker* biologici che rifletcono differenze nella gestione del calcio intracellulare. Al di là di ciò, si può etichettarli come entità molecolarmente e geneticamente distinte. Ci si chiede se questa diversità sia un residuo dell'evoluzione o se abbia un significato funzionale specifico. La caratteristica *fast spiking* è probabilmente dovuta alla presenza di questo *buffer* del calcio. Nei neuroni con accomodamento, il calcio ha il tempo di accumularsi e, raggiungendo una certa soglia, attiva delle conduttanze che tendono a iperpolarizzare il neurone. Questo non accade nei neuroni *fast spiking* perché il loro *buffer* è più veloce.

Inoltre, i neuroni *fast spiking* sono spesso identificati come *basket cell* e proiettano principalmente con sinapsi inibitorie sui dendriti dei neuroni piramidali, almeno nel *layer* 5. Le cellule di Martinotti, invece, che mostrano accomodamento, non hanno una particolare orientazione del dendrite apicale e presentano una proiezione più complessa, indirizzata al soma o in posizione più prossimale rispetto al soma.

### La Sfida della Semplificazione nei Modelli Neuronali

Tutte queste caratteristiche vengono probabilmente perse nei modelli che trascurano tale diversità e, ad esempio, distinguono solo tra neuroni eccitatori e inibitori. È davvero importante questa specificità, o è semplicemente un modo in cui la natura, attraverso meccanismi biologici particolari, deve specificare nel genoma "tu proietta a questa classe di neuroni nel soma" oppure "proietta alla stessa classe di neuroni ma in una parte distale"?

Tuttavia, potrebbe anche darsi che questa complessità, pur essendo un residuo dell'hardware biologico sottostante, possa essere funzionalmente rappresentata come un neurone inibitorio che proietta a un neurone puntuale con un peso sinaptico ($W$, come nei pesi sinaptici delle reti neurali artificiali) che assume un certo valore per un tipo di neurone e un valore molto più grande per un altro tipo cellulare. Non è quindi chiaro come si possa passare da questa diversità e complessità a una semplificazione efficace.

In alcuni casi, in particolare per la morfologia, esistono addirittura connessioni *ad hoc*, dove, ad esempio, alcuni neuroni inibitori proiettano esclusivamente a specifici neuroni eccitatori. Tra di loro, inoltre, possono esserci altre caratteristiche di proiezione specifiche. Ci si chiede, dunque, se queste siano delle "imperfezioni" (*bugs*), residui lasciati dall'evoluzione, oppure delle "caratteristiche" (*features*) funzionalmente rilevanti. Per il momento, non riusciamo a stabilirlo. L'unico modo per stabilirlo potrebbe o dovrebbe essere quello di replicare artificialmente un modello che si comporti in modo efficace, dal punto di vista degli *input* e degli *output*, allo stesso modo del sistema biologico. Si tratta, in fondo, di qualcosa di simile a un test di Turing, che, come forse abbiamo brevemente menzionato l'altra volta, imita la realtà in modo che, dal punto di vista di un osservatore o di una neuroprotesi, il comportamento sia indistinguibile dall'originale.

### Livelli di Organizzazione e Approcci di Modellazione

Tuttavia, questa attenzione per i dettagli si manifesta anche nei diversi livelli di organizzazione del sistema nervoso. Forse vi ho mostrato una *slide* simile l'anno scorso, che illustrava una gerarchia di organizzazione tra molecole, cellule, cellule connesse da sinapsi, popolazioni di cellule con sinapsi, sistemi, l'intero cervello e il comportamento (*behavior*). Ciascun livello riflette diverse tecniche e approcci alla modellistica.

È possibile modellare le singole molecole, ad esempio con tecniche di dinamica molecolare, dove una particolare proteina, con i singoli atomi che costituiscono gli amminoacidi, può essere descritta con principi primi basati su meccanica quantistica ed elettrostatica. In questo modo, si può osservare l'assemblaggio delle molecole o la formazione del poro di un canale del sodio. Il problema è che non si riesce a simulare per più di qualche microsecondo; anche facendo girare un supercalcolatore per settimane, si rimane su una scala temporale non paragonabile a quella del comportamento. E così via: quanto più si mantiene il dettaglio a un livello precedente, a un certo punto la complessità esplode, rendendo impossibile la descrizione anche dal punto di vista computazionale.

Andando in questa direzione, si aumenta l'astrazione e, dal punto di vista delle approssimazioni, si tende a farne sempre di maggiori, accontentandosi di una descrizione fenomenologica non più puntuale, biofisica e ad altissima fedeltà. D'altro canto, scendendo di livello di organizzazione, la complessità aumenta e il numero di parametri esplode. Se in un modello a livello dell'intero cervello, come già detto, ci si può cavare d'impaccio con qualche centinaio di nodi, non si ha la possibilità di descrivere la concentrazione di ioni sodio o potassio nell'ambiente extracellulare. L'ambiente extracellulare non è proprio presente nell'equazione e nella descrizione matematica. Se si tenta di reintrodurlo, aumenta il realismo biofisico, ma aumenta anche il numero e la quantità di dati necessari per validare tale approccio.

### Falsificabilità e Semplificazione dei Modelli: Prospettive Storiche e Filosofiche

Dal mio punto di vista, sono particolarmente affascinato da due figure. Karl Popper, filosofo della scienza, sosteneva che la validità di una teoria scientifica fosse intrinsecamente legata alla sua possibilità di essere falsificata. Ciò significa, in altre parole, che se una teoria ha troppi parametri e include tutte le caratteristiche possibili, con migliaia di parametri da *fittare*, come nell'astrologia ("sì, ma avevi Saturno in opposizione in quel momento e l'ascendente era quello, quindi questo spiega perché stamattina ti sei svegliato alle 4 e non alle 6 come avresti voluto"), essa è difficilmente falsificabile. Popper affermava che una teoria è tanto meno falsificabile quanto più è complessa e ricca di parametri. Al contrario, una formula come $E=mc^2$ ha una variabilità di parametri estremamente limitata e una struttura semplice (un prodotto, una potenza), il che la rende chiaramente verificabile o falsificabile.

Mi piace anche ricordare in questa *slide* Louis Lapicque, che nel 1907, quindi addirittura prima di Hodgkin e Huxley, fu il primo a proporre un modello molto semplificato dell'eccitabilità neuronale, che studieremo estesamente: il modello *Integrate and Fire*. È un modello che, nella sua logica di semplificazione e riduzione della complessità, assume che il potenziale d'azione sia sempre uguale a se stesso, sebbene abbiamo visto che anche in un modello multicompartimentale non sia così. Ciò che conta è il momento in cui il potenziale d'azione viene emesso. E poiché il potenziale d'azione è un evento stereotipato e più o meno sempre uguale a se stesso, si può scegliere di non descriverlo in dettaglio. Per queste ragioni, accomuno questi due personaggi.

### Pionieri della Modellistica Multicompartimentale

Tuttavia, il contesto in cui ho studiato e lavorato negli anni include altre tre figure carismatiche. In particolare, Wilfrid Rall, uno scienziato americano scomparso da qualche anno, è stato un pioniere nell'uso dell'equazione del cavo e nell'utilizzarla per descrivere l'integrazione sinaptica, l'eccitabilità e la funzione dei dendriti, della morfologia e delle biforcazioni (*branching*). Idan Segev, suo allievo, è attualmente il massimo esperto di modellistica multicompartimentale che incorpora la morfologia. Un altro scienziato con cui ho lavorato, di cui ho preso il posto ad Anversa, in Belgio, e che ora si trova a Okinawa, in Giappone, presso l'Okinawa Institute of Science and Technology (OIST), un'istituzione con risorse finanziarie illimitate, è stato anch'egli, come Segev, uno dei pionieri della modellistica multicompartimentale. È stato il primo a sviluppare un modello matematico delle cellule di Purkinje, che, come menzionato, sono estremamente complesse, riccamente arborizzate ma piatte su un unico piano, e sono cellule fondamentali del cervelletto.

### Strumenti per la Modellistica Multicompartimentale: Neuromorpho.org e ModelDB

In definitiva, se la morfologia e la diversità biofisica sono importanti e se l'inclusione di questi dettagli è rilevante, allora occorre misurarle. Vi ho già introdotto il perché e il come si possa acquisire una morfologia digitale. Tuttavia, rimane il problema: una volta ottenuta, è difficile eseguire una simulazione come quelle che vi ho mostrato l'anno scorso per il modello di Hodgkin-Huxley, dove con il metodo numerico di Eulero si gestivano quattro equazioni differenziali (una per il potenziale $V$, e una per le variabili di *gating* $m$, $n$ e $h$). In quel caso, il metodo di Eulero era sufficiente.

Quando in un modello matematico si cerca di inserire una componente spaziale, rappresentata da "cavi" (come descritto in precedenza), con numerosi compartimenti, ciascuno dei quali deve essere descritto con le proprie equazioni, diventa oneroso scrivere il proprio codice da zero. I personaggi menzionati poco fa hanno affrontato questa sfida, iniziando a scrivere codice per risolvere modelli che non si limitavano a un singolo punto (il soma), ma includevano numerosi compartimenti. Questa è la cellula di Purkinje e questo è Cajal, di cui abbiamo già parlato.

L'idea è: se ho la morfologia e la biofisica identificate, almeno a livello del soma, come discusso in precedenza, come si simula e si studia un modello del genere? Questo presuppone che un laboratorio sperimentale mi fornisca tali informazioni, oppure che io possa attingere a un database. Esiste un database molto importante di morfologie, Neuromorpho.org, dove si possono "fare acquisti" di morfologie neuronali. Ad esempio, su Neuromorfo.org, se si cerca per regione cerebrale, come il bulbo olfattivo accessorio, e si desidera la morfologia di una cellula granulare, si può trovarla. La morfologia della cellula è disponibile sul *web*; il sito può persino mostrarla in 3D tramite un'applet interattiva (anche se a volte non funziona bene o è "crappy"). È possibile scaricare la digitalizzazione, che rappresenta punto per punto ciò che lo sperimentatore ha dedotto. L'applicazione *web* la visualizza in 3D non solo per scopi di visualizzazione, ma per consentirne l'utilizzo numerico in una simulazione.

### Il Simulatore NEURON

Il modo più o meno standard oggi per simulare e studiare le conseguenze di questi modelli multicompartimentali — che presentano un'equazione per ogni segmento, utilizzando di fatto l'equazione del cavo discretizzata ulteriormente in singoli compartimenti, segmento dopo segmento, *branch* dopo *branch* — è basato sulle conduttanze. Ciascun compartimento, soma incluso, è descritto da conduttanze voltaggio-dipendenti che modellano l'attività passiva e attiva, ovvero le proprietà elettriche dipendenti o meno dal voltaggio.

Il modo per incorporare e simulare un modello di questa complessità, di cui non scriverò una singola equazione per esteso, è più facilmente rappresentabile graficamente. Ad esempio, se si dispone della morfologia acquisita da Neuromorpho.org, e i singoli canali ionici sono disponibili su un altro database (non Neuromorpho, ma ModelDB – vi fornirò i *link* su Teams). ModelDB permette di visualizzare, ad esempio, un modello del bulbo olfattivo realizzato dal collega Michele Migliore. Questo modello, che è in Python (anche se non era quello che volevo mostrare), utilizza una corrente di potassio di tipo A, che è una corrente voltaggio-dipendente.

È possibile scaricare il file contenente la descrizione quantitativa delle equazioni che descrivono la corrente del potassio come conduttanza moltiplicata per la *driving force*. In questo caso, non c'è l'esponente $n^4$ come per un potassio *delay rectifier*, ma la cinetica è descritta quantitativamente (ad esempio, $n_{\infty}$). Al di là della sintassi di questo strano linguaggio di programmazione, è possibile "fare acquisti" delle singole conduttanze che altre persone hanno misurato e che sono disponibili come *plug and play* per un simulatore chiamato NEURON.

NEURON, di cui parleremo la prossima volta, offre un'interfaccia Python in cui non è necessario scrivere le equazioni differenziali. Si può semplicemente usare comandi di alto livello, come `create soma` (dove `soma` è il nome del compartimento, ma potrebbe essere qualsiasi altro nome). Poiché in NEURON ogni elemento è un cilindro, questo compartimento deve avere una lunghezza e un diametro, ad esempio di 10 micrometri; è come una sfera, ma rappresentata come un cilindro con altezza e larghezza uguali. Al suo interno, dal punto di vista biofisico, si inserisce solo una corrente di *leak*. Esiste quindi un simulatore che, per un modello con capacità di membrana e corrente di *leak*, accetta comandi ad altissimo livello. Non è necessario implementare il metodo di Eulero o risolvere equazioni differenziali come $C \frac{dV}{dt} = \dots$ numericamente. Dando questi comandi in pasto al simulatore, quando si simula uno *step* di corrente, si ottiene la curva di carica e scarica esponenziale attesa.

### Esercitazioni Pratiche con NEURON

Concludo dicendo che nel *repository* del corso, sul sito "Modeling Blabla", nella sezione "Risorse", sono disponibili, oltre alle *slide*, dei *notebook* con cui si può iniziare a sperimentare. Questi *notebook* consentono di installare NEURON nel *cloud* e, potenzialmente, anche sulla propria macchina locale, poiché le risorse richieste non sono eccessive. All'interno della cartella "notebooks" è presente anche un file intitolato "how to install Neuron".

Se desiderate installarlo sul vostro computer, sono disponibili istruzioni. In alternativa, potete continuare a utilizzare il *cloud*. In ogni caso, il codice che vi ho menzionato diventa facilmente modificabile da voi, spero, come primo passo per giocare con questo tipo di simulatore. Con una decina di righe di codice, che includono anche la specifica di voler *plottare* il potenziale di membrana ($V$), la forma d'onda della corrente di stimolo e i valori temporali, è possibile eseguire una simulazione in pochi istanti. In questo caso, si tratta di un singolo compartimento, ma la prossima volta vi mostrerò come sia possibile, ad esempio, creare un modello *ball and stick*, che include non solo un soma ma anche un lungo dendrite, con cui potrete sperimentare modificandone la lunghezza.

Ci fermiamo qui. Vi invito a provare a sperimentare e, per pura analogia (non voglio che studiate il manuale di NEURON), potreste provare a cambiare o aggiungere qualcosa. L'uso di *tutorial* e di strumenti come ChatGPT o Gemini nel *cloud* è semplice; potreste utilizzarli a vostro vantaggio per chiedere, ad esempio: "Supponi di voler creare un modello con una biforcazione, anziché un *ball and stick*; come potrebbe essere implementato?" o "Cosa succede al punto di biforcazione?". Tuttavia, è fondamentale prestare attenzione e cercare di comprendere il codice proposto, poiché "allucinazioni" e soluzioni di codice eccessivamente complicate sono all'ordine del giorno. Sono sempre a vostra disposizione per domande, dubbi o ulteriori curiosità. Buon *weekend*.

