% Processed Transcript: ../course_website/ModellingNeuralSystems/overheads/transcripts/Class7_raw.md
% Date: 2025-12-11

## Spike Frequency Adaptation

After a difficult return to the airport, where I was supposed to return in a more timely manner, I actually arrived on Sunday morning. I would like to resume the part about adaptation, specifically spike frequency adaptation. This is very important because, as I will show you in about an hour, or in about twenty minutes, complementing the excitability mechanisms described in a reduced way with the Integrate and Fire model, even a Leaky Integrate and Fire model with this adaptation becomes capable of fitting experimental data. In other words, it becomes possible to achieve agreement between simulations and experiments, a quantitative agreement. Therefore, it is a component that different neurons express in different ways. Given the formal simplicity of the Leaky Integrate and Fire model, which, as a reminder, has an equation that the subthreshold membrane potential satisfies, an external synaptic current, for example, this expression holds when V is below the firing threshold. It is called "leaky" because this term is a loss term. It also reminds us of the behavior of a flow from a container full of liquid. This minus sign, being here the rate of decrease, for example, the rate of appearance or decrease of a certain quantity V, which could be the level of a liquid in a tank, with a term minus proportional to the level, explains what would physically happen if you had a physical pipe, a drainpipe, a hole in the famous bathtub, and so on. This applies when V is below threshold. Otherwise, if $t^*$ is the moment when the threshold is crossed, meaning $V(t^*) = \theta$, then for all times in the interval $t^* + t_{ref}$, where $t_{ref}$ is the absolute refractory period, it is more complicated to write formally in mathematics than to write, for example, in code as I showed you a few weeks ago. During all these times, $V(t)$ is held clamped at a certain hyperpolarized, constant value, which, for simplicity, we can consider equal to the resting potential, or the weighted average of the Nernst potentials of the various ions to which the membrane is permeable. It has become just a number. However, it continues. This, I mentioned, is an advantage of the Integrate and Fire model. It continues to be not only biologically motivated and realistic but, above all, compared to other types of models that we will see later today, it has a potential correspondence with the membrane potential, with the subthreshold trajectory over time of a potential that you can measure in a real neuron. If this were the membrane potential $V_m$, you could have a one-to-one agreement. Paradoxically, as in the case of real neurons, at a certain point you could compare the moment when action potentials are fired, are emitted, in the mathematical model and in the experiment. Other types of models no longer have the action potential, as mentioned. Often, the analogy with thermodynamics is used, and the fact that thermodynamics describes a system like an ideal gas with a single observable, for example, it calls it temperature T, not the kinetic velocities $v_i$ of all the billions of molecules of oxygen, nitrogen, and whatever else is in this room. A single value describes the entire accumulation. Something similar is done, and it will be the subject of these and the next lectures, in the case of a spiking neural network. In that case, I abandon the possibility not only of describing the membrane dynamics of a single neuron but of all of them. I consider it like thermodynamics considers a single observable for many individuals, and I also abandon the possibility of talking about spike timing, the moment when an action potential was emitted. However, there are other advantages. Let's see what the characteristics of the Integrate and Fire model are. Given that this dynamic is relatively simple from a mathematical point of view, so much so that in some numerical simulation techniques, this subthreshold differential equation can even be solved, it can even be implemented in a way that does not require the use of Euler's numerical method or other more sophisticated methods. It can even be exploited by the fact that... So this equation here, if the synaptic input is particular, typically synaptic input, you know, and we will review it, is normally zero when there is no presynaptic event, and then it could be, for example, a very rapid increase. I am talking about an IPSP or an IPSC. These are currents, on the right side of this charge balance equation, so they are currents, an inhibitory postsynaptic current (IPSC) or an excitatory postsynaptic current (EPSP) respectively. So, since this trace is normally zero, this term is zero, I could paradoxically implement this system numerically with a type of analytical solution. Obviously, I have to iterate it over time, but here I have to iterate it instant by instant using Euler's method. If I had a clever way to exploit the fact that when the synaptic input is zero, I know the analytical solution and can calculate it analytically from here to, in effect, infinity, to whatever I want. Obviously, the solution would no longer be valid upon the first arrival of a synaptic input. So, although this is beyond the scope of this course, there are simulation techniques called event-driven, in which the acceleration you can give to a simulation of several million Integrate and Fire units can be dramatic, because you no longer have to simulate, to integrate the equation instant by instant; you can effectively jump from one spike to another, which is very powerful. However, this is not sufficient to capture the dynamics of a real neuron. It must be complemented with this, with this dynamic. I think your colleague, who could look this up online, instead of seeing me wave my hands at the blackboard, might... he doesn't like it, he doesn't like it now, yes, but okay. So, it complements the dynamics of this equation with a term, theoretically, an additional current term. This is one of the possibilities. I told you, and I was very, very quick last time, that it is even possible to simplify things further. One of you, before lunch, asked me if X, in this case, must have the meaning, or has the meaning, of calcium concentration. It could also be intracellular sodium, as this dynamic equation describes in a highly simplified way what is an electrodiffusion equation, in which, however, I have thrown everything away. There is nothing left here; there is only a kind of phenomenological description. There is no longer a calcium current, for example. This could be the change in free calcium concentration in a single compartment, in a point-like compartment. No problem, this description of the Integrate and Fire model is also a description of a point neuron. So, I have abandoned the idea of having multi-compartmental structures. In this case too, I have an ordinary differential equation. Here, fundamentally, I have a first-order term that reminds me, recalls a first-order kinetics. But in fact, it is saying in a highly simplified way that if I wait a certain time, a timescale that is usually a few tens, hundreds of milliseconds (100, 200, 500 milliseconds), the intracellular calcium, the intracellular sodium, spontaneously decreases, goes to some steady-state value that can be zero, imagining that there are mechanisms that pump out calcium ions from inside the membrane. Remember that there is practically zero calcium inside; there is very little, while there is a lot outside, as for sodium. But I don't describe how this happens; I just say that after waiting a while, there is no more calcium inside. And again, it represents in a highly simplified way what should actually be on the right-hand side of this equation, a term with a plus sign. This one with a minus decreases; it describes how calcium decreases. This one with $E^+$ would be the current during an action potential. And again, in a Leaky Integrate and Fire model, I don't have the action potential at all. I told you it's an aesthetic issue. So, here the potential never goes above the threshold. If the threshold is -50 mV, then theoretically, I should put a calcium current here, which is normally 0 and becomes non-zero when the potential is depolarized, because it is a voltage-dependent current, like for sodium, fast-inactivating. It has the same characteristics: the channels start to open when the potential is depolarized. Here I don't have it. I even say that only during an action potential there is an influx of calcium, and I describe it in a pulsed way. This is what remained of the story of my current account, which I will show you shortly with the numerical implementation, the demo. So, this is a further, extremely simplified treatment of intracellular calcium dynamics. I always say calcium or intracellular sodium because there are currents that are calcium-potassium dependent or sodium-potassium dependent. In both cases, they do the same thing: they are potassium currents, so they tend to push down, tend to oppose excitability. They are sensitive, in their own right, to an intracellular accumulation of calcium or sodium. The fact that it is sodium is because, like calcium, it accumulates with electrical activity. People have experimentally described the presence of both types of currents. This expression here, in some way, correctly bothers your colleague because it is an extremely barbaric approximation, barbaric in the sense of poor, dramatic, obscene. A term that I would have put here, and all those nice things about accuracy and biophysical realism, okay, I should put this type of current here. And this type of current, where they will have some maximum conductance, then will have some state variable here. I imagine it's just 'm', I don't think there's inactivation, or I think there is, but this variable 'm' satisfies a Markov kinetic scheme, as complicated as you please, so that it's just a single... I don't do the trick that worked well, but there are three, and then the potassium potential, and with $K^-$ the membrane potential. Here, I have even put 'x', and your colleague said, but I put the concentration... at most this 'm', if you allow me this notation, $m(1-m)$, to say open, closed. In reality, it could be more complicated. That 'x', calcium concentration, if these channels are not voltage-dependent, are not calcium-dependent, or sodium-dependent, that 'x' should enter into the alpha and beta rates, which are no longer functions of the potential, perhaps they are no longer just functions of the potential, they are also functions of the calcium concentration. So, for example, here 'x' could characterize the functional dependence of this beta rate that opens this potassium channel. This would be the correct, even more physical description, that you have in multi-compartmental models based on conductances. And it would be correct, but you understand that here I have to add even further differential functions, $dm/dt$, etc. In the end, if you remember our discussions last year, for which, yes, okay, if in this kinetic scheme it is effectively a first-order equation, you engineers think of it as a black box where the output roughly follows the input, apart from some kind of delay. Let's say 'm', even though again, it's a sword, and it's in a style that might bother you because it's an approximation of "stoppiness," of being negligible, we would say 'x' are the same thing. I understand that you might say, "No, okay, from a temporal point of view, yes, but 'x' is millimolar." Okay, within the units of 'x', there will be some term that, in terms of units, has something like Hertz divided by millimolar, so maybe a linear term simplifies, but 'm', beyond the unit laws, 'm' is between 0 and 1. So much so that I put it here, and I have that at most the conductance is $g_{bar}$, it's the upper limit. Here I have even put 'x', which is millimolar. This, in theory, does not have a limit of 1, does not have a kind of saturation. However, in the context of choosing the simplified Integrate and Fire model, an argument like, "Yes, it's true, I should put the potassium reversal potential here, but I put the resting potential, the potassium one would be -80 mV, I put -60 mV, I don't care anyway, roughly the model would do the same thing, you can try." Quantitatively, things change a bit, but quantitatively the effect is the same. And also here, if instead of 'm' I put 'x', again the behavior qualitatively does not change. So, what you could correctly point out, as purists, is, "But if 'x' becomes much greater than 1, 1 is no longer 1, it's a fraction, it's 1 millimolar, it's something that has the dimensions of a concentration." It's as if I had a carefully curated biophysical description with 'm', with 'Mh' or whatever it is, in which I no longer have saturation. So I have to imagine that the two worlds are more or less equivalent qualitatively when I can be far from saturation. Perhaps something even more dramatic could be that since this term here, V, especially in an Integrate and Fire model, doesn't change, and $E-V$ is always a negative quantity because it's -60 mV minus -50 mV, okay, V at most is between... it could go below -60 mV due to some effect of inhibitory synapses, which I am modeling here. But in the range V between E and $\theta$, this quantity is still negative. What I could do is throw away the dependence on V, the driving force. So you could be further disgusted and say, "But how, this is not even dimensionally a current anymore." It's true, but it would allow me to write here a term minus, apart from a scaling factor, which here again incorporates $g_{bar}$, incorporates a term that is certainly more reminiscent of $E_{mean} - V_{mean}$. Okay, it no longer changes as a potential, but beyond the dimensions, and here I could write that it is something like some constant that I call K times X. So, in a simple, linear way, because in the end I remain scared from the point of view of mathematical complexity, I have a way to have an equation that is, in fact, a linear equation, apart from being only subthreshold, but it is a linear equation with constant coefficients. And this is also a linear equation with constant coefficients. Now I have two, and they are coupled by a linear term. All this could favor my attempt, like the frequency-current curve that we saw together the other time, to be able to do some considerations with pen and paper. In the case of this, this is a negative feedback. It's a feedback because it depends on how much more potentiation there is, the more this expression is visited, is used, from the point of view of numerical code it is executed, the more X grows, and the more X grows, the more this term with the minus weighs, and it tends to disfavor the firing of other action potentials. So, this is what you saw, I will show it to you now live, you saw it also from a graphical point of view, in a simulation where here you have spike frequency adaptation, you see that the rate is lower for calcium because there are only calcium-potassium dependent currents. There are not all combinations. There is not, for example, the sodium-potassium dependent current. It has not been described. One can think about the fact that some proteins, probably from an evolutionary point of view, there was a need in biology to create some feedback mechanism, either positive, which engineers call amplification, or negative, which they call regulation. In fact, they don't call it regulation, but for clarity, on the one hand, from a phylogenetic point of view, meaning evolution across all species, on the one hand, it has generated, in particular, this is something very well known from a biochemical point of view, and something I may have mentioned last year, in the olfactory bulb or also in metabotropic receptors, there is this whole cascade of biochemical reactions, one coupled to the other, which implements exactly this mechanism of amplification or regulation of adaptation. I believe that with phylogenetic evolution, evolution had to find another, much faster way, and therefore membrane channels that normally had a three-dimensional conformation that changed with the electric field, which in itself, if you think about it, is a very complicated, very strange thing to have evolved, because it strikes me that information coding occurs through electrical mechanisms. Perhaps you might say, "Okay, no, once there are ions in aqueous solutions, and ions have a charge carrier, perhaps for free, if you have access to the world of electronics, of 'iontronics' or whatever it is, of electromagnetic fields." But having channels that open or close when, in the intracellular, cytosolic part, there are calcium ions that attach or detach, this is further surprising. Perhaps it has not only evolved for this type of regulation, but if you think about synaptic transmission, vesicles in the famous "kiss and run," they fuse with the intracellular part of the synaptic bouton on the cytosolic side and fuse, giving rise to exocytosis, when intracellular calcium increases. So, there is never calcium inside, so when there is, it means something happened. This something is the opening of calcium channels. In the case of synapses, it is precisely what I want: when there is an action potential, voltage-dependent calcium channels let calcium in. Here it is more general; I am not describing a mechanism that is even more complicated than vesicle fusion with the membrane. It simply opens a channel, a channel, therefore a pore, which, however, does not change with the membrane potential, but changes with the presence of ions that bind, activate, excuse me, bind in appropriate pockets from the inside. They bind, obviously, and they bind. Something similar perhaps evolved for the NMDA synaptic receptor, which, if you remember, has the possibility from the extracellular side for magnesium ions to get stuck and remain stuck. There it even has an electrostatic component, so when the membrane is more positive inside, it is ejected. Here, no. Here it is just a sensor, and by chance, there is a lot of calcium, because if there is a lot of calcium, I rely on these mechanisms in the end. It means there has been a lot of recent activity. So, there might not be a purpose to detect another ion. First of all, I would start with an ion of which there is very little intracellularly. And intracellularly, there is little sodium and calcium. Sodium, you remember, if you lick your sweat, it's salty, because the usual thing is disgusting. So, there is little sodium inside. Chlorine should be the same; there is a lot outside and little inside. So, only sodium and calcium remain. Perhaps magnesium, but I don't remember the concentration offhand. I believe there is no magnesium, there is no difference between intracellular and extracellular magnesium. So, ergo, no, it only applies to calcium or sodium. So, the variation of calcium is a reference point between the intercellular. Correct, it is the intracellular calcium, absolutely. In this way, I repeat, I simplify my life and mathematically have a coupling, a negative feedback, and two simple coupled equations. I will show them to you now. You have the... On Google Colab, you have the simulations, possibly to play with. This is the simulation that I showed you last time. Simply by changing the current, the subthreshold membrane potential integrates the current, behaves like a passive RC circuit (blue trace), but obviously, once it exceeds the threshold, it is no longer a passive RC circuit. Then there are these stereotyped events where the potential is held, is reset, and is held at a reset potential for a certain period. This, in the end, was trivial. This discussion, I have already shown you this, about refractoriness. Here there is only absolute refractoriness. Obviously, it doesn't work, so okay, when I applied the famous two current pulses to the same neuron to get two action potentials, then I brought them closer, reduced the time between the two stimuli, at a certain point one of the two disappears because this neuron, I don't see it here, but it continues to be refractory. I don't see it because the reset I chose is at the same level as the resting potential. If I had chosen, as is done in some cases, an even more hyperpolarized value, I would have seen that at a certain point here it would have been held down and then would have returned to the resting potential. However, if at a certain point the second spike disappears, and there is no way, even if I pump the current of the second pulse to very high levels, there is no way to make it fire, because I rigidly ignore what is the integration of the current, and I am here and I say, if it is still refractory, okay, keep V clamped at that value. So, in fact, this could, from the point of view of sophistication, of accuracy in describing an experimental situation, even a Hodgkin-Huxley model, which, as I recall, was conceived for the giant axon of the squid, not for the cortical neuron of a mouse or a human neuron, however, has notable advantages that we will see shortly. Here I have put, again, the invitation is to try it. You have the code, so as a possible exercise in the slides that I will show you later, there is my request to calculate and plot the frequency-current curve. Then try to implement the equations that were here before the adaptation and try to see how it changes. I will show you now, so here you have the code, but you could choose not to look at it immediately, to try it yourself. You could try to... simply try it. If you have problems, you can ask me. If you try and it doesn't work, you can ask me again. And it can be useful, despite the fact that typically there are simulators, like the NEURON simulator, which we briefly saw, there are simulators that, for example, easily simulate Integrate and Fire models. But for once in your life, since the equations are very simple, you could implement them yourself. Here, with this slider, I control how much, so for each dot, it's a simulation like this. And T is the duration of the simulation. I do nothing but count how many action potentials have occurred. Since I have to plot the frequency, I take this number N and divide it by this time T. Last time, or perhaps two or three weeks ago, I mentioned the fact that the shorter this time, the shorter this time, 150 milliseconds or 110 milliseconds or 90, or whatever it is, you see that the frequency estimation degenerates. What is actually happening is that I have a phenomenon that would normally occur at a certain... here the current is constant, it is to be understood as constant current. So this is a pacemaker. It always has a fixed inter-spike interval, which we can even calculate analytically. The black trace is the plot of that function with the logarithm. I remember the logarithm, because I told you that for free, or perhaps not for free, it is a consequence of this equation, of the fact that there is an exponential here and the fact that it is the inverse of the exponential of the logarithm. But for free, one has that the theoretical, analytical frequency-current curve is defined only for currents above the baseline area, the minimum current for sustained firing. Because, in fact, the argument of the logarithm is not defined when it is negative; in the domain, it is only defined for positive arguments. So, by itself, it would have this regular inter-spike interval. And I observe it for a time that can be very long, or a bit shorter, or very short. Here, if the inter-spike interval changes, it changes continuously. You see the black trace, the black curve, is the analytical curve, and the frequency is changing continuously. It's an ugly function, it's not that ugly. The logarithm of that thing, I don't remember it by heart.

## Spike Frequency Adaptation

There was a part in the numerator inside the logarithm that had terms for both current and $g$, and also the threshold. So much so that if, again, the current were not sufficiently large to yield a positive argument for the logarithm, the logarithm would not be defined, and therefore the frequency would not exist. Thus, this inter-spike interval changes continuously. If I increase the current slightly, these intervals also shorten slightly. If I observe this over a very short time, the time will always be short. Ideally, one would perform a simulation of infinite duration, but nobody has that much time, not even free time, because in that case, I would be able to appreciate the small changes. Here, for example, in this case, within this small interval, I always see three action potentials. If these become closer and closer, it doesn't immediately become apparent. In this interval, I always see three. I divide by $t$ and obtain what is a frequency of, for example, 60 spikes per second. For a change to occur, this integer, this natural number, must become 4. And before it reaches 4, I must continue to increase the current. I simply notice it later because this is a small observation domain. So, for example, this must pass a bit of time; in the sense that I don't see it immediately. I must increase the current quite a bit before seeing a change, because this number goes from 3 over $t$ to 4 over $t$. If, instead, I had a very, very large observation time, I would have many, many more spikes. Suppose I might have 16,876 divided by $t$. Here, if the inter-spike interval decreases slightly, perhaps these become 787, and I can, the observation, the observation window is so wide, I see this change immediately, it increases immediately. Here, no. And in fact, this staircase behavior you see here, particularly evident here, is precisely due to this, $T$ is too low. If I make it a bit larger, or even considerably larger, the simulation takes more time, obviously, because I am doing it 24 times. So I take the integrate-and-fire, like this trace above. So I set a current that is -0.5; there are no spikes, I mark 0. I change the current with some step; now it's 0 spikes. I increase the current; no spikes. I do this repeatedly, and instead of plotting the trajectory, I plot only the number of spikes $n$ divided by the duration. And you see that compared to the theoretical expression, the dots, which are the estimate $n$ divided by $t$, are a way to evaluate the frequency, tend to become, they are no longer a staircase, and they tend to become increasingly accurate in predicting the theoretical expression. I am certain here that the theoretical prediction is correct because I derived it, because it works, obviously, under the assumption that the input current is constant, but it is the same constant current. The only thing that happens, you see, now I set it to 2300, I set it to 4000, the dots deviate less and less from the trace. What remains, obviously, is numerical noise, there might be numerical inaccuracy, but the dramatic change between this condition and this condition should be appreciable. The other thing this slider does, if you want to play with it, is simply to increase the number of simulations, thus making the step with which the current is increased denser. So here it's 24 points. If I set it to 44 or more points, I do it because I am interested in understanding what happens around the threshold, the baseline area, and I would be interested in knowing it because I have the idea that at the threshold there is obviously a discontinuity. There isn't a discontinuity like in the Hodgkin-Huxley model, which is a type 2 excitability where the frequency jumps instantaneously. However, I might want to ascertain this numerically. So every time I increase the number of points, it obviously takes much longer to simulate, I obtain ever greater accuracy in describing this value at the threshold. What I know, using the analytical formula, is that there is a change right there; so there is no discontinuity, but at least there is a discontinuity of the first derivative, meaning the curve starts quite steeply here. And I might ask myself, if I perform an experiment on a neuron, as soon as I go above the baseline, it immediately starts firing, and is the frequency-current curve that steep? In reality, no, it has a more linearized trend, graphically speaking, from a graphical point of view, this is a linearized trend, and it is precisely due to the effect of frequency adaptation, which I will show you, sorry, of frequency-dependent adaptation. This is the same small model I showed you a little while ago. Obviously, to confuse you, I changed it, unintentionally, I changed it, I copy-pasted from an old notebook, converting it from Julia to Python, so that in theory, despite the conceptual aspect, you might say, "Okay, I'm not familiar with Julia, Python, or MATLAB, but roughly, I can intuit what a certain piece of code is doing, especially if Michele Giuliano writes the code variables comprehensibly, if he doesn't call them, if he doesn't write ad hoc code to confuse you." However, you also already have GPT, Gemini, and so on, to explain a line of code to you. In this case, these are not particularly dramatic algorithms. If you put this notebook into GPT or Gemini, it continues and asks, "Do you want me to plot this other thing too? Do you want me to plot the curve?" and it talks about computational neuroscience, so how much material it has ingested, particularly from GitHub and scientific articles on the same topic, so it's not surprising. Here, the different thing is that the equation, when there are no action potentials, also has this minimum term, a minimum value, which was previously set to 0. And I wanted to show you exactly what would have happened if I had... this is the history of the current account. If I had... but perhaps I should restrict it a bit. So I simulate three seconds of simulation time with some time step. I don't know why I put this in. And what I can do is I can increase or decrease the frequency of these events, which simulates, which emulates the firing frequency of the neuron itself. You see that when the frequency is low, the tails are completely exhausted. The next event, my famous current account, I repeat, I don't spend monthly in proportion to... so the equation for $x$ is $dx/dt = -\beta x$, or I wrote it another way, and it was the same thing, there was a $\tau dx/dt = -x$. I don't spend when the university doesn't pay me my salary. I spend; this is my change, the variation over time. I don't spend, I don't buy on Amazon proportionally to my account balance. It's true, if I did that, I would never go negative because when $x$ reaches 0, I don't spend, I don't have an order that I can think of on Amazon that would have a sign of 0, would have a value of 0. In the case of my expenses, there should be a constant, I don't know, -580 euros. Clearly, here, this would tell me that I could go into the red, indeed, I can go negative in the case of a bank account, but I would no longer have tails. This constant term, integrated, gives me -580 times, in the case of this quantity which is proportional to how much calcium there is, it has a tail and saturates, to put it more elegantly, instead of being a linear term $-t$ or a decreasing exponential $e^{-t}$ blah blah, and which tends to zero as $t$ tends to infinity. If the frequency increases, you get that nice phenomenon where the events tend to build up, to arrive on the tails. You see here that the tail of the first one is not yet finished, and so on for the tail of the second one, but at a certain point, you also see that practically immediately you have a kind of dynamic equilibrium, in the sense that that band no longer changes. This is clearly visible, and this band tends to increase with frequency. So, the higher the frequency, you see that the band takes roughly the same time constant, the same order of magnitude to rise. However, this distance from zero to, for example, the midpoint, or the peak, or this point at the bottom, tends to increase as a function of frequency. If I set an even higher frequency, this band is not very appreciable, it increases. One way to make it even more appreciable is to slow down the tails. If I make the tails ten times slower, immediately, so it's as if I had to fire at a very low frequency to avoid building up on the previous tail. But this way it's seen very well. So I changed the $\tau$ or $\beta$, and here it really goes up, it's really clear. So there are two influences: one that tends to cause a loss, in this case too, a loss, and the influence of new calcium injection. In the specific case, what I wanted to show you was the triviality of changing. So, for example, let me put it back as it was, 0-1, for example, give some summation. And if you change that minimum concentration parameter, instead of having 0, I set it to 2, simply everything translates upwards. So, okay, I have a kind of baseline that is higher, but conceptually, it's not particularly interesting. The interesting thing from a conceptual point of view is this growth. If you put it into the integrate-and-fire again, here, for the umpteenth time, to confuse your ideas, I called it $w$ instead of $x$, and here at the end I didn't put another coefficient, but I think there's an error because in the code, in the function where there are integrate-and-fire, there must be some if, then, else, if I'm above threshold, or if I'm still refractory, or if I'm not. I was looking for this equation here, which is the right-hand side, because the Euler method requires me to evaluate the right-hand side of $dV/dt$ anyway. But no, okay, here too it appears with $-V$. Well, okay, so that $G_{bar}$ I mentioned earlier, saying rightly that it bothered your colleague who expected to have a nice current like $G_{bar} M$ driving force, here it even has a unit coefficient. I'll show you what changes by changing, because it shows me the code, because I have to do it this way. So when the current is sufficiently high, I keep it like this, changing $B$, which is the amount of calcium injected each time. Again, here they become parameters; I'm no longer saying what one identifies experimentally. Experimentally, I could measure $G_{bar}$ and I could measure $\alpha$ and $\beta$ in an ad hoc voltage clamp experiment. Here the style is different; I say, "Okay, I have the real experimental trace, no longer single channel, but the membrane potential trace. What happens if I change $B$?" I can change it to the point where, indeed, here it's so strong that you see it deforms, it's a hyperpolarizing component that deforms the membrane potential profile. If $B$ were very small, it would do nothing. Let me change $B$ so that the step can be 0.2, so that I increase the possibility of changing. You see that if $B$ is too small, now if you squint, you can see that the first inter-spike intervals are very short, but the frequency is always very high. If I change $B$, which is a component of this negative feedback, you see that the slowing down is much more consistent. Here I have the frequency, even the initial frequency, but especially the final frequency. So while before it fired very fast, now it gets tired and fires very slowly. Another way to see the same thing, clearly with a different effect, is to change the time constant with which calcium is extruded. So if I make it faster, I lose memory of the past, and so you see that there is no longer adaptation. If I make it slower, it means I remember the past much more. If I set it very, very large, perhaps you can appreciate that the transient changes, so this part here changes, beyond obviously the frequency. I remember, for the same frequency and feedback, that there is pre-existing activity before taking the break. Let me show you instead what happens to the frequency-current curve, continuing to indicate in blue, I should have done it in black, no, here too it was in blue, here too it was in blue, if I don't space, sorry, here how was it? Blue, black, how was it? It was blue, okay, the theoretical curve of the frequency-current expression is in blue, and you see that if I set $B$ equal to 0, $B$ equal to 0 means I no longer have any jump, no more calcium current injection, the university no longer pays me, or rather, they pay me, but they pay me 0 euros per month. The behavior of the integrate-and-fire with adaptation is indistinguishable from that without adaptation because, yes, you have less $k x$ there, but $x$ always stays at 0. If you want, it could stay at a minimum value, this offset value that I told you about unintentionally, inadvertently, confusing you, changing it, calling it a bit $x$, calling it a bit $c$, whatever it is, $C_{min}$. As soon as I start putting $B$ different from 0, what you see is that the curve bends, and it bends because I am considering, in effect, the behavior predominantly at steady state. It is true that during the transient, the neuron fires, perhaps for 2 or 3 spikes, at the same frequency as when there were no adaptation currents because the buildup hasn't happened yet. It's as if I had just arrived in Modena, my bank account balance was zero, and therefore for the first two or three months, I don't feel the effect of my account balance. It continues to exist, it bends immediately, and I can show you that even if I make this adaptation mechanism have a short memory, I have a smaller and smaller deviation. If I make it such that the past history matters a lot, or that the frequency-current curve bends. Before taking a 10-minute break, I just want to tell you, the fact that it bends is something you should appreciate intuitively, because it simply means that if before, without adaptation, I did this and kept hitting the table at a certain frequency, now I rest. So, at the same current, while before I was firing at that frequency, at 60 spikes per second, now I fire at 10 spikes per second. So obviously, if you do this for every frequency, you discover that the neuron still maintains a proportionality of the response to the current; it doesn't ignore the current. With a lower current, perhaps it would have fired like this, and adaptation would have only slowed it down a bit, not particularly, for example, here. So it's not that adaptation is a subtractive quantity; it's a quantity that remains proportional, it's a scaling, it's more of a scaling factor. And the interesting thing is that here the threshold, beyond the fact that it's very dramatic here because all the points are very bent, so to do something slightly more, I would need something more realistic. What is this $\tau$? 100 milliseconds. This could already start to be realistic, 50-100 milliseconds are... You see that here the behavior at the baseline is a bit less nonlinear, a bit less steep, and therefore, from a computational point of view, as seen in machine learning, the transfer function of the units, if it's enough to be threshold-linear, but not necessarily to have a discontinuity, a very high nonlinearity, it seems to work, it seems to allow the various classifiers of deep networks to classify between dogs and cats or in transformers to predict the next word. Biologically, it might be that this adaptation mechanism, beyond metabolic reasons, costs too much to fire at 60 Hz for 10 seconds. So it might be that some cells, because not all of them have adaptation, excitatory cells, perhaps precisely those that during an epileptic crisis, given that their job is to excite, so if I fire and have a synapse with one of you, my target also starts firing because I release glutamate and excite everyone. Perhaps also as a mechanism to switch off, what is called runaway excitation, so uncontrolled excitation. So metabolism, maybe I get too tired, I need more ATP to sustain the high frequency. Second, if I am excitatory, as it seems to predominantly occur in excitatory neurons, not in all of them, sorry, not only in excitatory neurons, I have a safety mechanism. If, even if a neuron in the brainstem is removed, anyway, if it goes crazy, it's also funny that a neuron goes crazy, it might make sense that there is a mechanism that tends to moderate the activity to avoid creating an epileptic crisis. Third, perhaps computational reasons of linearity and nonlinearity of the expression. Excuse me if I have exceeded, we will stop for 10 minutes and then continue until the end, which is at 4 o'clock. Okay, that's it. Okay. Thank you. Thank you. And you say, we have the same background because I know... Enough, enough, because? My life isn't there. Thank you. Is it a bit of type that ever said mine? It's normal, it's a guest, it's a guest, it's black human, it's very black. Thank you. Thank you. Yes, because we are a bit born. Eh, but like, how many would we have? Yes, but in March we are... One hopes, eh, always. Well, before a year. Anyway, we are on Friday, what... to go. Yes. Yes. No, no, no. If you want the post, I feel like I'm there. There's a bit of being there. So, how do you think there's a bit of being there? Yes, there's a bit of being there. Yes, there's no. Thank you. Thank you. If you want my love, I'm thinking. Thank you. Thank you all. One of you is missing. Okay, then I renew the invitation. Forget the extensive final discussion, comments. Forget it. But if you can, try, as for a practical exploration part that I invited you to undertake with the previously provided, in fact, with the Neuron simulator, here I invite you to write the simulation code yourselves. And implement and simulate an integrate-and-fire, for example, with a constant current or with an arbitrary current that you can provide. You have it on Google Colab on our GitHub repository, you have the code, but you could try to ignore it and see if the code I wrote is not perfect, is not particularly elegant, or is commented. So maybe some of you suffer from a bit of OCD, Obsessive Compulsive Disorder, maybe they say, "No, this is written, but I want to do it well myself," then maybe you give it to me, I use it for your successors, particularly to simulate a realization of a membrane potential trace over time, the frequency-current curve, for example, compared with the analytical formula as I showed you, and again, discuss, we did it together, the differences between theory and simulation. What doesn't happen particularly because it wasn't particularly evident with integrate-and-fire models, which are very simple, but the more you have an accurate frequency-current curve with a lot of points on the x-axis, a lot of current values, the more the simulation is heavy. You have to wait a bit of time. Every day for fast computers, not even for, even if languages like Python are interpreted, so they are poor in terms of performance, you realize less and less how much more complicated the model is, how much more you want to do a parameter exploration like in this case, I want to change, I want to see what happens to the current threshold. Each time I do a different simulation, it might convince you to say, "Okay, it's a compromise. If I want to give a graph because I want to show it to Michele Giuliano, I have half an hour, so I can't leave it." You might leave the computer all night, but this doesn't happen for integrate-and-fire. It might make you realize, "Okay, yes, a computer is not enough for me; in theory, I would need a supercomputer, etc." Here, with adaptation, the version with adaptation and the effect on the frequency-current curve, note that if you know how to write the frequency-current curve, I don't know why I wrote it differently. Ah, okay, because this is the formula $F$ as a function of $I$. If you know how to plot, if you have code that does this parameter exploration, I try a current and start a simulation, then I reset the initial conditions, change the current, restart the simulation. Once you have it, changing even, changing the model itself, is less of a hassle. If the model is different, if it's not a leaky integrate-and-fire as we saw together, or another family, or if it has adaptation and doesn't have it, the part of the code, the script of the code that calculates and plots the frequency-current curve is the same. And so this thing about the effects of adaptation near the baseline, of linearization, is interesting. Adaptation far from the baseline, we haven't talked about it, you could think about it. Here are parameters you could use; otherwise, you can look at those in the code. It's not necessarily true that these are the real parameters of a neuron. Now I'll show you, taking up some slides that I've already briefly commented on, that these parameters could have an effective interpretation. So I do an experiment and I want to see if, by chance, the frequency-current curve of the leaky integrate-and-fire fits, identifies, approximates, does exactly the same thing as the frequency-current curve of a real neuron, and to do that, I have to move these parameters. It might not have the same meaning; it might typically, for example, this value here, in the experiments I show you, the best fit became tens of milliseconds, despite the fact that I know that biophysically the absolute refractory period is 1-2 milliseconds, not 100 milliseconds. But in effective terms, only when you put 150 milliseconds here and other values, then you have a 1-to-1 match with the experiment. This was to show you how adaptation bends the frequency-current curve. Here are other parameters, another zoom, and here the simulation duration is evidently a bit too short. In the individual simulations, I indeed have this staircase effect. And so I'm picking up something I mentioned a few weeks ago, telling you that in the past I was also involved in the experimental validation of these models. The fact of having a simple model in physics has become a matter of DNA and a reductionist approach. That is, the fact of having an extremely complicated model like multicompartmental models based on conductances with thousands of parameters to identify might not be the most correct way for science to proceed. If you want to build a bridge, you are civil engineers; you might want to favor a simplified, perhaps effective understanding, not perfectly, not at the level of individual calcium ions extruded, but something that tells you roughly, if you want 20 meters, how thick the extruded calcium should be. The same thing in theory, in physics, I repeat, it has now entered the DNA of normal treatment. So if I can write for a pendulum, which is the favorite example, and I can write the formula for the period of the pendulum, and I notice, for example, that it doesn't depend on mass, it depends on the length of the string, I could build a clock with a different period. So there it is very powerful. If, instead, I start describing with all the dynamics, so it's not a point mass but a rigid body, it's not a string but an elasticity, yes, okay, fine, no, but there's also air friction, and then, anyway, look, the hook doesn't behave ideally; there's friction, it dissipates heat. Okay, you can do it; you could obtain, as mentioned at the beginning of this course, something that is as complicated as reality. And so where can you learn? Where can you extract some considerations? I repeat, from a bioengineering point of view, it could be useful: how to build a bridge? How do I change the stimulation current parameter today to stimulate what remains of the sensory cells of the cochlea? You could tell me, "You increase the current until you burn the tissue." If you burn the tissue, it means you are injecting too much current, and the patient is there; ask yourself if they are perceiving anything.

## Spike Frequency Adaptation

Ok, this did not necessarily have to go this way. The type of waveform is guided by concepts of biophysics. The fact that it must be a balanced waveform is because at the interface, there are redox phenomena, and you want the reactions to be reversible. Therefore, as much negative charge as positive charge must be balanced. Otherwise, some chemical species that you are generating at the interface will be lost, and you will be generating some waste that could, in the long term, change the behavior of the cells. So, the scientific or theoretical approach is not an end in itself.

In this case, we prepared slices, which are the typical preparation used to study intact neurons of the cortex, particularly in this case, the auditory cortex. We focused on certain cells, of which you see the soma here. This figure, this slide, I have shown you a thousand times, even last year: pyramidal cells. Specifically, we made coronal slices, not sagittal ones. We made a slice that is oriented this way and not that way because we were interested in taking a part of the cortex that, in rodents, is internal, between the two hemispheres, and is called the medial prefrontal cortex, or also medio-prelimbic cortex. This was because it was interesting to see if these cells had a different behavior, simply from the point of view of their frequency-current curves. And the resource indicated yes, they are different. So, why? First, I tell you that the frequency-current curve is different. If you want, for example, to study a mechanism, a network of neurons, by comparing the somatosensory cortex to the medial prefrontal cortex, you could benefit from having a description based on the integrate-and-fire model and changing a couple of parameters. In one case, you would have frequency-current curves that go downwards, and in another case, curves that go upwards. So, again, I complement the experiment with an in silico component.

This type of experiment allows for different cell types. Here you see a pyramidal cell again, which you classically study in the usual way: I inject a constant current, and the neuron fires. Here, you might see it poorly, but this cell is at the top; it is the response of a pyramidal cell, and it exhibits frequency-current adaptation. You can see this from the first two inter-spike intervals. Here, the adaptation is relatively fast, so presumably, the time constant for calcium extrusion is quite short. If it were not, the effect would propagate, and the transient would be longer. However, you can realize that adaptation certainly plays a significant role because if you compare it with the response of another neuron from the same area, from the same cellular layer, layer 5, a neuron that is an inhibitory neuron called a basket cell, you see that here, first of all, with the same current, the same current amplitude, which could be in the order of 100-200 picoamperes for half a second, for 500 milliseconds (it's not written here, but it's around 500 milliseconds), you see that the frequency here is much, much higher. It's perhaps 10 times higher, or certainly 8 times higher, so much so that you might not be able to see the individual spikes at this distance. It's like squinting your eyes; you see roughly the same thing here. It's blurred, but here you appreciate that there are many more. So, for the same current, it's as if the frequency for that cell were more... So, this is the constant current. For the same current, here is the pyramidal cell, and here, instead, is the fast-spiking cell. It's called fast-spiking because it fires faster, while pyramidal cells are also called regular firing adapting, meaning they fire regularly and, in addition, they adapt. So, it is conceivable that while pyramidal cells behave like this, fast-spiking cells have a much greater stiffness, a much greater steepness of the frequency-current curve. A much greater gain.

Those of you who have ever seen transistors will know that when you have an input-output relationship, here the output is the frequency, but it is exactly what then enters the stomach of other neurons. So, it is the computation that can be performed by that neuron. A transistor amplifies, or less so, depending on the slope, the slope of this transfer function curve. So, I am comparing neurons to transistors that could amplify or not amplify input signals. There are other cells that are purely inhibitory, called Martinotti cells, whose response to the same current is very sustained. However, here it is seen very well, perhaps at your distance, especially if you wear glasses and then take them off. Here, these cells show remarkable spike frequency adaptation, frequency-dependent adaptation. Indeed, you see that the density of spikes at the beginning of the impulse train is much higher than at the end. And for many cells, here each curve represents a different cell, you can plot the current on the X-axis, which you see is the same, the same range, and the frequency in Hertz, or spikes per second, on the Y-axis. It is preferred to say spikes per second because Hertz could also be more traditional. Although they are identical, the same thing is used to describe oscillations or frequency components in the Fourier domain. They are always periodic phenomena, but here it is a more immediate firing rate if one lists that as the unit of measurement. And you see that the slope of these pyramidal cells is rather low compared to the fast-spiking ones, the basket cells. And these, the Martinotti cells, are still steep, but they are more adapted; the slope is lower compared to the fast-spiking cells. And for many years, people, for example, this is an article from 2003, but there is one even from '85, have classified different cell types based on how they fire, on this slope. Okay, this one fires a lot and has a high slope; yes, they call it fast-spiking. This one, instead, no, they call it regular firing. This one, okay, they call it fast-spiking, but in reality, it's fast-spiking with adaptation; you can't see it from here. There are other cell types, but most people work with the frequency-current curve, and as I told you last year, it's a bit like a Rosetta Stone, a kind of partial fingerprint, a business card, but it's very widely used experimentally.

So, experimentally, it's okay, these are the experimental data. But if I take an integrate-and-fire model and give it the same current value, the same sequence of current values, meaning I first inject 0.1, then 100 picoamperes, then 150, 200, 250, 500 picoamperes, 700 picoamperes, and calculate the frequency, perhaps, as in simulations, I don't do it over 500 milliseconds, I do it over 30 seconds of stimulation. Exactly for the same reason that I would want to have an accurate frequency estimate, especially if the curve changes continuously. It's not exactly the same thing; it's not sampling like sampling a time signal. It's more about having a precision, a confidence in the frequency estimate, given that I inevitably estimate it with a limited observation of a process that yields an integer number of action potentials. And so, one can ask, what are the parameters that, if I put them into the integrate-and-fire model, allow me to capture exactly the shape of the frequency-current curve of a pyramidal neuron or an interneuron? This is what we did. It doesn't make sense now. Next week, and perhaps with the lecture we will make up next week online, I will tell you. I will put it on Teams, and as I said, I will make myself available on Teams if you have any questions. For a morning, I will keep myself freer and be in front of the computer, waiting in case you have questions. So, for the moment, ignore that there are different colors; they are different stimulation conditions. You can imagine that it is still an experiment where, at a certain constant current amplitude, I injected it for 30 seconds, 60 seconds, and measured how many spikes per second there were. The continuous curve you see, beyond the fact that there are three colors, three conditions, suggests that the integrate-and-fire model captures it well across all three colors, not just one of them. Apart from this point, which deviates slightly from the model's prediction that the neuron should not fire, all the others are not bad. In the eyes of an experimentalist, this is a good fit. There are also statistical methods to determine if this fit is of good quality or not. A statistical test, for example, is called the chi-square test. The chi-square test is often used to assess the goodness of a fit. That is, in all possible universes, have I been particularly lucky that now, having found these values for the membrane time constant, the refractory period, the capacitance value, the reset value, the adaptation value, and another one that is not written because it is implied, by some optimization routine, I was lucky that the experimental points were close to the theoretical curve, or not? These are statistical methods that allow us to say, yes, in all possible universes, this can only happen by chance in very few universes. So, it means I got it right. I cannot exclude the fact that this is a coincidence, that it is entirely by chance, that I had a stroke of luck, but this typically happens statistically very rarely. This is typically used in statistics, physics, and medicine as a criterion to determine how confident you are in your result. There is no way to know if this happens or not.

## Stochastic Processes and Neuronal Input

Now, the three conditions, we are starting to see it now. It's a slightly more tedious part of the course because it deals with a part of probability theory. There is nothing difficult to understand, and I will try to make it as simple as possible. In reality, in this experiment, we did not inject a constant current; we injected a constant current with a bit of noisy fluctuations. Technically, it is a stochastic process, and more precisely, fluctuations distributed according to a Gaussian distribution density. And blue, compared to red and green, is... another color. Green, for example, has the same offset, which is this value, this value; this is the mean, but it has much higher, much larger fluctuations. How do I imagine this? I tilt my head and say, why if I tilt my head and see that there are... I don't care anymore if there is temporal proximity. I take all the points, for example, all the blue points, all the red points, and I flatten them and make a histogram. So, here is an axis that talks about the current; here was the time; here is no longer time. It's another graph that I simply put on the right because now it might illuminate you or not. Here I put the number of times that in many bins, in fact, I am making a histogram, but I give you a somewhat more emphasized presentation. I ask myself how many times it happens that there are samples here, here, here, here, or here. How many? How many times does it happen that there are samples here? It never happened here, even at this value, and even at this value. So, for blue, the histogram is more like something like this, while for red, it's more like something like this. Now, obviously, I plotted them wrong. I plotted them wrong because the area under these probability density functions should be unity. So, intuitively, I made them have the same peak amplitude, but that's wrong. Not only did I have to make the red one much sharper because the area under it must be equal to the area under the blue one and any other area. All probability distributions must have a unit area. I will tell you about this in much more detail. So, it allows changing both the mean, the average value, often called $\mu$ as a Greek letter, and it also allows changing, and it's the color, the standard deviation, which is the square root of the variance. I call it standard deviation because it's particularly convenient. I don't care if it's twice where there's 68%. I don't care anymore. It's a measure that tells you how spread out the Gaussian is. And you know from probability theory that a Gaussian, a probability distribution density, is identified only by these two parameters, by these two moments: mean and variance. So, if I have any theoretical reason to say, "Neuron, I know that in vivo you certainly don't receive a DC current," I do this. The experimentalist does it, who first does it because the amplifier that has this does it, you press a button, and you inject a constant current. But you, neuron, in your life, never see constant currents unless I inject them. For physiological reasons, you see fluctuations. I, being an engineer, say, "Wait, so I systematically decompose mean and variance and move them independently." I am systematically moving the mean as I would have done before, and for the variance, in theory, it's a two-dimensional thing. I choose only three values. If I remember correctly, it's not written here, but blue is 50 picoamperes in amplitude, so very small, practically you see it's a noisy trace that is not DC. Red was 100, and green was 150 or 200 picoamperes. There are reasons, which we will start to see, that have to do with what is the regime that a real cell in my cortex is currently experiencing. These suggest saying, "Yes, I know I cannot give you an exact playback of the stimulus you were experiencing, cell. I inject a stochastic process that, from a statistical point of view, has similar properties." If I know your response to both the mean and the variance simultaneously, then things become interesting because I can predict what the behavior of a large network of similar neurons will be. So, the experiment here is more informative. It is the fact that the integrate-and-fire model, in which I did not just inject a deterministic signal, but in reality, I injected a realization of a stochastic signal. A realization means that I clearly have a random number generator. I will show you how to generate a stochastic process of this type. Perhaps some of you have white Gaussian noise in mind. I cannot do white; I cannot generate it, especially experimentally, because white means that two adjacent points of this signal are completely independent. It would mean that the fluctuation speed is infinite, and as you can see, I had an inertia in my muscles when moving my hand when I plotted this. So, adjacent points, unless you take a large interval, but adjacent points are correlated. This is called a process; it's not white but colored because the spectral density, the power, which means the Fourier transform of something, is not exactly of the trace, but let's pretend it is. It tells me that I don't have energy at all frequencies, which would mean infinitely rapid transitions. No, transitions up to a certain frequency. And physicists call this thing, because frequency and wavelength in the world of electromagnetism are called the color of light, here there is nothing colored, but it is called colored noise instead of white noise. From a theoretical point of view, however, you may have heard in the past, perhaps some of you, the concept of white Gaussian noise because with pen and paper I can define it, and then I cannot generate it. I cannot write a routine on the computer that generates it, not even for numerical simulation, that generates Gaussian noise. However, mathematically, I could say, if I have an integrate-and-fire model, so if this is the equation $C \frac{dV}{dt} = \dots$, if I put a signal here, a current, and I say this current is Gaussian and is white noise, perhaps with pen and paper I can understand what the statistical properties of $V$ are. That is, I can study a differential equation that is no longer called ordinary but stochastic. This stochastic differential equation, in the form I wrote it, is also called, and I'm not sure if I mentioned it last year when we talked about ion channels in their stochastic description, the Langevin equation. And it's an equation that I also showed you, that I used when I showed you the balls moving by diffusion, pretending they were diffusing. It's an equation that somehow encapsulates in stochastic terms what could be microscopic interactions. In the context of neuroscience, microscopic interactions are effectively communication between neurons. From a macroscopic, mesoscopic point of view, I speak of stochastic processes that I inject because somehow you can move towards a simplification, a description that goes from microscopic to mesoscopic. It will become clearer, and so I promised myself to try to explain it by adding a preliminary part of probability theory that you might not have particularly fresh. Again, it's not a course in probability theory, but it might be useful to have a refresher. Note, probability theory. I have said statistical properties many times, but I think of probability theory. Probability theory is a deductive theory that goes from the general to the particular. That is, I know what is particular. I know, for example, that $x$ has this amplitude, that it is Gaussian with that mean and that variance, and I can calculate the probability of observing that value now. This is probability theory, and I like it very much because it descends from fundamental principles. And having general statements, I can ask, what is the probability that, given these general statements, if I toss a fair coin 5 times, I get 5 heads? Probability theory tells me how frequently that could happen. Statistics, on the other hand, is unfortunately a nasty beast, particular. Perhaps it is not a science, even though it is the basis of machine learning and artificial intelligence. It proposes to go from the particular to the general, and you understand that it is almost impossible. That is, I observe a limited reality and try to generalize. So, this is deduction, and this here is called inference. And Compagnoni, I believe you should try something like that. I don't like statistics. And so, again, the reflection part I would like to propose does not have to do with statistics. Statistics is all a bit of a mess because it is almost impossible to infer general concepts, theories, or behaviors if I observe. Maybe I am very good. If I spend 50 euros in a couple of months of experiments, I can generalize. It's a big mess. Show me all the photos of kittens and dogs on the internet. How many will there be? Millions, millions, millions. But they are all cats and dogs. Maybe you can still generalize. In that case, machine learning models exploit what is called non-parametric statistics. So, if you have any recollection of probability theory and statistics, know that I am in the world of probability theory. Because it is from theory that I am saying, "Okay, theory, tell me how to generate a stimulus that has those fluctuations," not the other way around. It's not that I look at a neuron, and now I'll show you a recording, I look at a recording and say, "Ah, so surely these were the theoretical conditions underlying this behavior."

This gentleman is called Steriade. He died some years ago. He was a pioneer of in vivo electrophysiology. He was one of the first to make recordings in living, behaving, non-anesthetized animals. He was not a butcher; everything was done according to the rules of animal experimentation. And he wrote a book that I have, and I also have it in electronic format. I would never show it to you because, obviously, it would constitute an infringement. He titled it "The Intact Brain or Sliced." The type of experiment I showed you, this type of experiment is called brain slices. So, you make 300-micrometer slices of the brain. This gentleman, who obviously made many discoveries, particularly regarding the cortex, the functioning of the cortex during sleep, during anesthesia states, claims, it is even written in the book, that you must be cautious when talking about the intact brain and the sliced brain because, obviously, when you slice it, you are slicing it from many points of view. Mechanically, spatially is one of them. You are making 300-micrometer slices. The brain is a few tens of centimeters. My brain, I think, is less than two fists. So, a 300-micrometer slice by half a centimeter is quite small. It has the consequence of cutting many connections. Furthermore, compared to the intact brain, slicing means you are also thinking of being able to study it only for a limited experimental duration of a few hours. But my brain has been functioning like this for 51 years. And in particular, he was one of the first to show that during, I will show you this slide in particular, taken from a seminal article of his from 2001, if you are skilled enough to place a pipette in a neuron of the cat's visual cortex, this is a neuron reconstructed, I believe, using a silver impregnation method, with the usual Golgi method that Cajal used. Here, shortly before, Steriade and his collaborators had an electrode, not in the soma of this neuron, and they were able to simply measure the intracellular potential by seeing, in this trace below, that it was a continuation of what seemed like random fluctuations, and every now and then there was an emission of a spike. The animal is not doing anything. I am not presenting anything to its visual field. It is looking at a white screen. I am not saying that white is not stimulating the visual system, but there is nothing... The animal is not engaged, not involved in any task, nothing is asked of it, and somehow it looks at a white screen, and its membrane potential fluctuates as if it were a random walk. So, making random, probability theory, perhaps the two things can have... or at least what it seems, it seems like random behavior, and the emission of spikes is not regular as I have presented it to you until now. It seems unpredictable, it seems more, again, with the language of probability theory. I don't know if you have ever heard of point processes. A point process seems like something that happens instantaneously, similar to the probability that my cell phone rings now. And now, most of the time, it's zero. When it rings, it's a similar probability from the point of view of probability theory to the probability that a raindrop hits your head while you are walking. What you see instead, a moment later, a few hours later, after the animal's brain has been extracted, so the animal has been sacrificed, the brain has been extracted, it has been sliced very rapidly, in the same way we slice rat brains, not the exact same cell, but the same class of cell is measured on a slice. And what you see is a very, very different situation. A situation where, practically, the trace is flat. Yes, it's at the membrane potential, -60 millivolts, -70, or whatever it was, which last year I convinced you was absolutely essential to understand, and it is, to comprehend the functioning of the dynamics, the functioning of the brain and neurons. The difference is abysmal. Here, the trace you see above, which is a bit confusing and should be a different color, but at the time, printing a color figure in a journal was very expensive, you see the trace of the electrocorticogram from an electrode, therefore macroscopic, which was placed on the surface of the brain.

## In Vitro vs. In Vivo Neuronal Activity

The discrepancy between in vitro and in vivo neuronal recordings is striking. This difference arises because in vitro preparations, where brain slices are made, lack the rich synaptic input present in a living animal. While the neuron itself might be viable, it is effectively deprived of its natural environment, akin to being "private of a lot of input." This isolation leads to a significantly different electrical behavior compared to neurons in vivo, which are continuously subjected to synaptic bombardment.

### The Role of Synaptic Input

The noisy, fluctuating activity observed in vivo is largely attributed to this synaptic input. This is why, in computational models, synaptic current is a crucial term in the charge balance equation. The inherent noisiness of synaptic input in vivo drives the observed fluctuations and irregular spiking patterns. This necessitates the use of probability theory and stochastic processes to accurately model neuronal behavior in a physiological context.

### Experimental Evidence: In Vivo vs. In Vitro Stimulation

A compelling experiment, conducted in 1996 by Rodney Douglas, Christophe Koch, and colleagues, highlights this difference. They recorded from a layer 5 pyramidal neuron in the visual cortex of an anesthetized cat. When a non-noisy, direct current (DC) pulse was injected into the neuron in vitro, the response showed adaptation, characterized by a decrease in firing rate over time, with the inter-spike intervals increasing. This is a textbook example of the integrate-and-fire model's behavior.

However, when the same experiment was performed in vivo, the neuronal response was dramatically different and almost indistinguishable from the activity observed when no current was injected, but rather when visual stimuli were presented on a screen. The DC injection in vivo did not produce the characteristic adaptation seen in vitro. Instead, the membrane potential continued to oscillate and fluctuate irregularly, with spikes occurring in a non-uniform manner. This suggests that the DC current alone was not the primary driver of the observed activity.

The irregular spiking and membrane potential fluctuations in vivo, even during a seemingly "boring" and temporally flat DC injection, point towards an underlying factor that is present both during current injection and in its absence. This strongly implies that network activity, specifically synaptic input, is the dominant force shaping biologically realistic neuronal firing patterns.

### Miniature Synaptic Potentials in Slices

Even in brain slices, where overall activity is greatly reduced, closer examination of the membrane potential reveals small fluctuations. These are often echoes of spontaneous synaptic release events. While a slice is a simplified environment, with reduced sensory input and a limited number of neurons, synaptic connections are still present. Axons may be cut, but the presynaptic terminal and synaptic bouton can remain intact.

These synapses can release neurotransmitters spontaneously, leading to miniature synaptic potentials (or miniature excitatory postsynaptic potentials, mEPSPs, if excitatory). This spontaneous release occurs even without an action potential arriving at the presynaptic terminal. Similar to how ion channels exhibit "flickering" and contribute to membrane noise, synaptic vesicles can fuse and release neurotransmitters independently. In vitro, these miniature synaptic potentials are observable and can serve as an indicator of the slice's health. However, after several hours, synaptic transmission in slices becomes fragile and ceases, leading to a "flat trace" at the resting membrane potential, indicative of a comatose state.

The miniature synaptic potentials observed in slices are typically small, perhaps around half a millivolt. When multiple synapses are active, temporal and spatial summation can occur. If the frequency of these events is low, they might appear as distinct, albeit small, deflections. However, as the number of active synapses increases, these individual events begin to overlap, creating a fluctuating, irregular current that resembles a stochastic process.

### Blocking Synaptic Activity to Isolate Intrinsic Noise

To definitively determine the source of these fluctuations, experiments involving the blockade of synaptic transmission are crucial. By applying a cocktail of antagonists to AMPA, NMDA, and GABA receptors, researchers effectively "deafen" the neuron to synaptic input, whether evoked or spontaneous. In such a condition, if the observed fluctuations were due to intrinsic neuronal properties, they would persist. However, experiments show that in the absence of synaptic activity (after applying antagonists), these miniature synaptic potentials disappear. This strongly suggests that the observed fluctuations in membrane potential are not primarily due to intrinsic channel noise but rather to synaptic activity.

### The TTX Experiment: Isolating Synaptic Input

A particularly elegant experiment involves the use of Tetrodotoxin (TTX), a potent blocker of voltage-gated sodium channels. TTX prevents action potential generation and propagation, thereby inhibiting synaptic release. In vivo recordings often show significant noise and irregular spiking. When a hyperpolarizing current is injected into a neuron in vivo, the resulting voltage deflection, which is inversely proportional to the input conductance ($ \Delta V = I / G $), is typically smaller than expected in vitro. This suggests a higher input conductance in vivo.

The hypothesis is that this increased conductance is due to ongoing synaptic activity. To test this, TTX is applied locally to the vicinity of the neuron being recorded. If the hypothesis is correct, blocking synaptic transmission with TTX should lead to a significant reduction in membrane potential fluctuations and an increase in the apparent input resistance (or a decrease in input conductance).

Indeed, experiments show that after TTX application, the noisy, irregular activity observed in vivo is silenced. The membrane potential fluctuations diminish, and the response to injected current becomes more akin to the in vitro, adapted response. This indicates that the "casino" of synaptic activity is the primary driver of the irregular firing and membrane potential fluctuations observed in vivo. The reduction in input conductance observed in vivo, compared to in vitro, is attributed to the constant bombardment of excitatory and inhibitory synaptic inputs that effectively "shunt" the injected current.

### Quantifying Synaptic Input

The convergence of inputs onto a single neuron is substantial, with pyramidal neurons receiving input from thousands of other neurons. Even at a low spontaneous firing rate of presynaptic neurons (e.g., 5 Hz), the sheer number of synaptic events occurring within a short time window (e.g., thousands of events in 20 ms) creates a complex, fluctuating synaptic current.

When these individual synaptic events (e.g., AMPA receptor-mediated EPSPs) are summed, they transition from discrete, small deflections to a continuous, noisy fluctuation. As the number of overlapping synaptic inputs increases (from tens to hundreds or thousands), the individual events become indistinguishable, and the summed current approximates a Gaussian process.

This statistical property allows for a theoretical prediction of the mean and variance of this fluctuating synaptic input. By measuring parameters such as the frequency of spontaneous presynaptic firing, the decay time constant of individual postsynaptic potentials (PSPs), and the amplitude of individual PSPs, one can estimate the statistical properties of the total synaptic current.

### Replicating In Vivo Conditions In Vitro

This understanding allows researchers to artificially recreate the in vivo synaptic environment in vitro. Instead of injecting a simple, non-realistic DC current, a stochastic Gaussian current, whose statistical properties (mean and variance) are informed by theoretical predictions based on measured synaptic parameters, can be injected. When this is done, the neuronal response in vitro becomes remarkably similar to the in vivo recordings, exhibiting irregular spiking and subthreshold fluctuations. This approach validates the hypothesis that synaptic input is the primary determinant of realistic neuronal firing patterns.

### The Role of Synaptic Fluctuations in Network Dynamics

The irregular, fluctuating nature of synaptic input has profound implications for network dynamics. It suggests that neurons in vivo are not simply responding to a constant drive but are constantly being perturbed by a dynamic and noisy environment. This stochasticity is not merely an artifact but a fundamental aspect of neural computation, potentially enabling functions such as enhanced sensitivity, efficient information processing, and the generation of complex network states. The apparent change in membrane conductance observed in vivo, which is reversed by TTX, further underscores the dominant role of synaptic activity.

## The Role of Synaptic Fluctuations in Network Dynamics

The essential characteristic is that if you use an integrate-and-fire model, and you introduce noise into it, you can derive an analytical formula similar to the one we found for the deterministic case, where the current stimulus amplitude was a step function, constant. This formula relates, even though it remains a stochastic process, the number of spikes per second, denoted as R (or R0 for historical reasons), to the parameters of the noise. When a threshold is crossed, the potential is reset, but the firing rate can be expressed as a mathematical function of the mean and the standard deviation of the input. In the deterministic case (when the noise standard deviation, $\sigma$, was 0), this expression simplifies to the logarithmic function we previously derived. Here, it even depends on $\tau$, the "color" of the noise, which characterizes the temporal correlations of the fluctuations. This allows for experimental validation by comparing the results with the model's predictions.

### Experimental Validation of the Integrate-and-Fire Model with Noise

By analyzing the firing rate, we can observe how it changes not only with the mean input current (the X-axis in our previous plots) but also with the amplitude of the fluctuations (represented by the blue, green, and red lines). Beyond just the firing rate, examining the subthreshold membrane potential histograms under different conditions of mean and fluctuation amplitude provides further insight. In the provided examples, which are histograms of membrane potential from four different cells studied years ago in Bern, the spikes have been removed to focus solely on the subthreshold dynamics. Calculating these histograms, which can be done with a single line of code in Python (e.g., using `numpy.histogram`) or MATLAB (`hist` or `histc`), reveals the distribution of membrane potentials. Although the input is Gaussian, the reset mechanism and the removal of spikes lead to a non-trivial output distribution.

This distribution changes significantly when input parameters are altered. When the input fluctuations are small, the distribution of membrane potentials is narrow. As the fluctuation amplitude ($\sigma$) increases, the distribution broadens, indicating larger fluctuations in the membrane potential. The black lines in these plots represent the predictions of the integrate-and-fire model, demonstrating that it accurately describes the subthreshold behavior in this "in vivo-like," noisy regime.

Further experiments on different cell types, shown here with continuous lines representing the integrate-and-fire model's response and dots representing experimental data, corroborate these findings. The experimental data points include error bars, which reflect the confidence in the estimated firing rate based on the duration of stimulation and recording. Longer recording times increase confidence. These experiments were conducted by Pascal Darbon, a former collaborator from Bern, on cultured cells, not brain slices. Alexander Rauch, who later worked with Logothetis at the Max Planck Institute, also performed similar experiments on fast-spiking cells. These cells are recognizable by their high firing rates, around 100 Hz on the Y-axis, compared to pyramidal neurons which typically fire at 10-20 Hz. Even with adaptations to the integrate-and-fire model, it can describe the behavior of diverse cell types, including pyramidal and fast-spiking cells.

In some cases, such as experiments conducted by Maura Arsiero in the medial prefrontal cortex, the firing rate response curves exhibit a different shape, bending over instead of continuing to increase with mean input current. This behavior is particularly interesting when considering the effect of fluctuations. Even with a wide range of fluctuation amplitudes (e.g., 50 mA to 200 mA), the firing rates converge at high mean input currents. This suggests that at strong mean drives, the neurons become less sensitive to the fluctuations, focusing primarily on the overall drive, analogous to listeners at a concert focusing on the main theme rather than embellishments.

However, other cells, like those found in the prefrontal cortex, show a different pattern where the curves bend, indicating a reduced interest in the mean drive and a heightened sensitivity to other factors, perhaps analogous to responding more to melodic embellishments than the main theme. This raises questions about the underlying biophysical mechanisms responsible for these distinct behaviors. The modeling approach, using integrate-and-fire models within larger networks, allows for the investigation of how individual neuron behavior scales up to network dynamics and how different neuronal properties influence network function.

